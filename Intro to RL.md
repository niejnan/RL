## Intro to RL

### Lecture 1:Overview(课程概括与 RL 基础)

![截屏2025-01-25 15.55.00](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-25 15.55.00.png)

RL 要解决的问题是：agent 怎么在复杂、不确定的环境中最大化它能获得的奖励。

在强化学习过程中，智能体与环境一直在交互。智能体在环境中获取某个状态后，它会利用该状态输出一个action ，这个动作也称为决策（decision）。

然后这个动作会在环境中被执行，环境会根据智能体采取的动作，输出下一个状态以及当前这个动作带来的奖励。agent 的目的就是尽可能多地从环境中获取奖励。



#### RL 与监督学习

监督学习的过程中，存在两个假设：

- 输入的数据应该是前后无关联的的。
- 需要告诉学习器正确的标签是什么，以至于可以通过正确的标签来修正自己的预测。

换句话说，监督学习的样本满足独立同分布。但是 RL 不同：

- 以 雅达利游戏 breakout 为例，游戏的画面作为输入，上一帧和下一帧之间有很强的联系。数据是基于时间的序列数据，不满足独立同分布。
- 在监督学习中，模型决定了输出，就会立即得到反馈，是正确还是错误。但是在 RL 中，例如下围棋，输入了此时棋盘的画面，机器确定了落子位置以后，并不能得到一个实时的反馈，只有当游戏结束了，才能知道这个动作是正确的还是错误的。换句话说就是，RL 无法得到实时的反馈。

某个时刻的动作，对游戏的输赢有没有帮助，这件事情是不清楚的，这就是 RL 中的延迟奖励问题。这使得 RL 的训练相当困难。

与监督学习不同，机器并不知道正确的 action 是什么，机器需要自己去学习哪些动作可以带来最多的奖励，只能通过不停地尝试来发现最有利的动作。



agent 学习哪些动作可以带来最多的奖励，就是一个不断试错的过程，**探索和利用** 是RL 中非常核心的问题。

**探索：**指尝试一些新的动作， 这些新的动作有可能会使我们得到更多的奖励，也有可能使我们扣掉一些 reward；

**利用：**指采取已知的可以获得最多奖励的动作，重复执行这个动作，因为这样做可以获得一定的奖励。

因此，RL 需要在探索和利用之间进行权衡，这也是在监督学习没有的情况。



#### 传统强化学习以及深度强化学习

![截屏2025-01-25 16.09.07](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-25 16.09.07.png)

对于传统的计算机视觉：

- 给定一张图片，提取它的特征，手工设计一些好的特征，比如方向梯度直方图（histogram of oriental gradient，HOG）。
- 提取这些特征后，训练一个分类器。以是SVM，然后就可以辨别这张图片是狗还是猫。

对于深度学习中的计算机视觉：特征提取以及分类在一起。

- 训练一个神经网络，既可以做特征提取，也可以做分类，可以实现端到端训练。

![截屏2025-01-25 16.11.42](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-25 16.11.42.png)

同理，神经网络 + RL = Deep RL

- 标准强化学习：标准强化学习先设计很多特征，这些特征可以描述现在整个状态。 得到这些特征后，可以通过训练一个分类网络 或者 分别训练一个价值估计函数来采取动作。
- 深度强化学习：一个端到端训练的过程。不需要设计特征，直接输入状态就可以输出动作。用一个神经网络来拟合价值函数 或 策略网络，省去设计特征这一步。



#### 序列决策：

在与环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。**历史(History)是观测、动作、奖励的序列：**
$$
H_t = o_1, a_1, r_1, \ldots, o_t, a_t, r_t
$$
Agent 在采取当前动作的时候会依赖于它之前得到的历史，所以可以把整个游戏的状态看成关于这个历史的函数：
$$
S_t = f(H_t)
$$


**状态**是对世界的完整描述，不会隐藏世界的信息。

**观测**是对状态的部分描述，可能会遗漏一些信息。

在 DRL 中，用向量、矩阵或者更高阶的张量来表示状态和观测。



#### MDP与 POMDP 的概念

**马尔可夫决策过程**

环境有自己的函数 $ s_t^e = f^e(H_t) $ 来更新状态，在智能体的内部也有一个函数 $ s_t^a = f^a(H_t) $ 来更新状态。

当 agent 的状态与环境的状态等价时，即 agent 能够观察到环境的所有状态时，就称这个环境是完全可观测的。

在这种情况下，强化学习通常被建模成一个 **马尔可夫决策过程** 的问题。

在马尔可夫决策过程中，$ o_t = s_t^e = s_t^a $。 

> agent 的观测 $o_t$ 与环境的真实状态 $s_t^e$​ 完全相同。
>
> agent 的感知状态 $s_t^a$ 与环境的真实状态 $s_t^e$​​ 完全一致。



FOMDP(Fully Observable)：完全可观测马尔可夫决策过程的假设条件：

**agent 对环境的状态拥有完整且无误的感知，不存在信息不对称或部分可观测的情况。**



**部分可观测马尔可夫决策过程**

在实际应用中，这种理想情况往往并不成立，因此会引入部分可观测马尔可夫决策过程(partially observable)：

- $o_t$ 与 $s_t^e$ 不再相等，agent 只能接收到部分观测数据。
- agent 需要通过推理或内部状态表示来估计环境状态 $s_t^e$ ，从而形成其感知状态 $s_t^a$​ 。

> agent 的观测 $o_t$ 与环境的真实状态 $s_t^e$​ 不一定相等，agent 的观测数据只是部分的观测数据。
>
> agent 的感知状态 $s_t^a$ 与环境的真实状态 $s_t^e$​ 不一致。

在这种情况下，强化学习通常被建模成部分可观测马尔可夫决策过程的问题。

部分可观测马尔可夫决策过程是马尔可夫决策过程的一种泛化。部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是假设智能体无法感知环境的状态，只能知道部分观测值。

比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。部分可观测马尔可夫决策过程可以用一个七元组描述：$ (S, A, T, R, \Omega, O, \gamma) $。

其中 $ S $ 表示状态空间，为隐变量，$ A $ 为动作空间，$ T(s' | s, a) $ 为状态转移概率，$ R $ 为奖励函数，$ \Omega(o | s, a) $ 为观测概率，$ O $ 为观测空间，$ \gamma $ 为折扣系数。



不同的环境允许不同种类的动作。在给定的环境中，有效动作的集合经常被称为动作空间（action space）。



**观测与感知的区别**

**观测（Observation）：**智能体直接从环境中获取的信息，通常是部分或有限的。通常用 $o_t$ 表示。

- 观测是外在的、直接提供的。
- 在完全可观测环境中，观测等于环境的真实状态。
- 在部分可观测环境中，观测是环境真实状态的一个子集或投影。

**感知（Perception）：**智能体基于观测以及历史经验或内部推理形成的对环境状态的理解。通常用 $s_t^a$ 表示。

- 感知是内在的，包含了智能体的内部状态和计算过程。

- 在部分可观测的情况下，感知通常是一种对真实状态的估计。

- 感知可能包含记忆、滤波或预测等过程（如卡尔曼滤波、RNN 状态更新）。

观测是智能体从环境中接收到的外在信息，而感知是智能体对这些信息进行处理后形成的内部状态或估计。

两者的关系就像输入和内部处理的关系，观测为感知提供基础，感知则支持智能体的决策和行为。



#### 策略(Policy)：

随机性策略：就是 $\pi$ 函数，即 $\pi(a|s) = p(a_t = a | s_t = s)$。

输入一个状态 $s$，输出一个概率。这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。

确定性策略：是 agent 直接采取最有可能的动作，即 $a^* = \arg\max_a \pi(a | s)$。

> 差别就是：
>
> 假设输入一个 $s$，模型的输出是，向左，不动，向右，得到的分数分别是0.6，0.2，0.1
>
> 对于随机性策略，直接采样，有60%概率向左，20%的概率不动，10%的几率向右。
>
> 对于确定性策略，argmax(action)，100%的向左，直接采取向左。

通常 RL 中会采用 随机性策略，这是因为：

1. 促进探索：随机性策略允许 agent 以一定概率选择不同的动作，而不是只选择当前看似最优的动作，因为当前的最优解并不一定是全局的最优解。
2. 适应不确定性：在部分可观测或动态变化的环境中，环境的状态可能并不完全可知，随机性策略有助于 agent 在面对不确定性时做出更灵活的决策。
3. 优化需求：随机性策略 通常基于概率分布，这使得 策略梯度 方法在优化时具有平滑性，避免因为非连续性或零梯度问题而无法更新。



#### 价值函数：

**价值函数的值是对未来奖励的预测，用价值函数来评估状态的好坏。**
$$
V_\pi(s) \doteq \mathbb{E}_\pi \left[ G_t \mid s_t = s \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k r_{t+k+1} \mid s_t = s \right], \quad \text{对于所有的 } s \in S
$$
这段公式定义了马尔可夫决策过程中的 **状态值函数** ，它表示在给定策略 $\pi$ 下，从某个状态出发，智能体期望获得的累计奖励。



**公式的部分含义**

- **状态值函数** **:** $V_{\pi}(s)$是一个标量值，表示在策略 $\pi$下，从状态 $s$ 开始所能获得的未来总奖励的期望值。

- **累计奖励** **:** $G_t$是未来的累计奖励，定义为：

$$
G_t = \sum_{k=0}^\infty \gamma^k r_{t+k+1}
$$
表示从 $t$ 开始的 第 $k+1$ 步获得的即使奖励， $\gamma$ 为折扣因子用于权衡当前奖励和未来奖励的相对重要性。

- **期望值** **:**$$\mathbb{E}_\pi[\cdot]$$表示在策略 $\pi$ 下，计算未来奖励 $G_t$ 的期望值。

- **公式的整体解释：**表示在策略 $\pi$ 下，从状态 $s$ 开始，智能体所能期望获得的未来折扣累计奖励。



**公式的含义**

**状态值函数的作用：**用于评估状态 $s$ 的“好坏”，即从该状态出发遵循策略 $\pi$ 能获得的期望奖励。一个状态的值越高，说明从该状态出发遵循策略 $\pi$ 的回报越好。

**折扣因子的作用：**折扣因子控制了未来奖励的重要性。较小的 $\gamma$ 表示智能体更关注短期奖励，而较大的则表示未来奖励的重要性更高。

> 假设智能体玩一个游戏：
>
> 1.当前状态 $s$ 是“站在迷宫的起点”。
>
> 2.奖励 $r_t$ 表示智能体在每一步获得的分数。
>
> 3.未来奖励 $G_t$ 是所有接下来可能奖励的加权和。
>
> 4.如果策略 $\pi$ 是“总是往右走”，则 $V_{\pi}(s)$ 就表示“从起点按这个策略出发，期望总共能获得的分数”。



**另一种价值函数 ：$Q$ 函数**
$$
Q_\pi(s, a) \doteq \mathbb{E}_\pi \left[ G_t \mid s_t = s, a_t = a \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k r_{t+k+1} \mid s_t = s, a_t = a \right]
$$

- **$Q_\pi(s, a)$**: 表示在策略 $\pi$ 下，处于状态 $s$ 并采取动作 $a$ 后的期望累积回报。
- **$\mathbb{E}_\pi[\cdot]$**: 表示期望值，其中期望是基于策略 $\pi$ 计算的，即按照策略 $\pi$ 选择动作的概率分布。
- $G_t$: 表示从时间步 $t$ 开始的累积回报。
- $\mid s_t = s, a_t = a$: 表示我们已经知道当前状态是 $s_t = s$，并且在当前采取的动作是 $a_t = a$。



换句话说，$Q_\pi(s, a)$ 是一个条件期望，计算的是给定状态和动作后，未来累积回报的数学期望。

累积回报的公式展开：
$$
Q_\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k r_{t+k+1} \mid s_t = s, a_t = a \right]
$$
**累积回报的定义**: $G_t = \sum_{k=0}^\infty \gamma^k r_{t+k+1}$，累积和表示从 $t+1$ 时间步开始的所有折扣奖励。

因此，公式的右半部分实际上是在计算：**在给定 $s_t = s$ 和 $a_t = a$ 的情况下，未来所有折扣奖励的期望值**。



“如果在状态 $s$ 下选择动作 $a$，然后按照策略 $\pi$ 行动，在未来我可以期望获得多少累积回报？”

- **决策依据**: $Q_\pi(s, a)$ 是强化学习中用来指导决策的重要依据，智能体可以根据它来选择动作。例如，通过选择具有最大 $Q_\pi(s, a)$ 值的动作（即 $a = \arg\max_a Q_\pi(s, a)$）。
- **策略评估**: $Q_\pi(s, a)$ 也用于评估策略 $\pi$ 的好坏。好的策略应该使 $Q_\pi(s, a)$ 的值尽可能大。



#### 模型

![截屏2025-01-27 23.48.23](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-27 23.48.23.png)

模型决定了下一步的状态，下一步状态取决于当前的状态以及当前采取的动作。

它由状态转移概率和奖励函数两个部分组成，状态转移概率即：
$$
p_{ss'}^a = p \left(s_{t+1} = s' \mid s_t = s, a_t = a \right)
$$
**状态转移概率**表示从一个状态 $s_t = s$ 经过一个动作 $a_t = a$ 后，转移到下一个状态 $s_{t+1} = s{\prime}$ 的概率。

> 如果地面滑而机器人有可能滑倒，状态转移概率 $p_{ss{\prime}}^a$ 可能不是确定的，比如：
>
> p = 0.8：机器人按计划成功移动到目标位置 $s{\prime}$。
>
> p = 0.2：机器人因为滑倒移动到了其他意外的位置。



奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即
$$
R(s, a) = \mathbb{E} \left[ r_{t+1} \mid s_t = s, a_t = a \right]
$$
**奖励函数**定义了执行某个动作 a 后能够获得的奖励的期望值。

它描述了动作的好坏程度，指导 agent 选择更优的动作。



有了策略、价值函数和模型三个组成部分以后，就形成了一个马尔可夫决策过程。



#### value-based 和 policy-based

基于价值的智能体(value-based)：显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。

基于策略的智能体(policy-based)：直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。



把基于价值的智能体和基于策略的智能体结合起来就有了actor-critic。agent 把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。

> 比较基于策略和基于价值的



#### 探索和利用

在 RL 里面，探索和利用是两个很核心的问题。

探索即我们去探索环境，通过尝试不同的动作来得到最佳的策略（带来最大奖励的策略）。

利用即我们不去尝试新的动作，而是采取已知的可以带来很大奖励的动作。



在刚开始的时候，agent 不知道它采取了某个动作后会发生什么，所以它只能通过试错去探索，所以探索就是通过试错来理解采取的动作到底可不可以带来好的奖励。

利用是指我们直接采取已知的可以带来很好奖励的动作。所以这里就面临一个权衡问题，即怎么通过牺牲一些短期的奖励来理解动作，从而学习到更好的策略。



与监督学习任务不同，RL 的最终奖励在多步动作之后才能观察到，考虑比较简单的情形：最大化单步奖励，即仅考虑一步动作。

即使在这样的简单情形下，RL 仍与监督学习有显著不同，因为智能体需通过试错来发现各个动作产生的结果，而没有训练数据告诉智能体应当采取哪个动作。



想要最大化单步奖励需考虑两个方面：一是需知道每个动作带来的奖励，二是要执行奖励最大的动作。若每个动作对应的奖励是一个确定值，那么尝试遍所有的动作便能找出奖励最大的动作。

更一般的情形是，一个动作的奖励值是来自一个概率分布，仅通过一次尝试并不能确切地获得平均奖励值。



**K 臂赌博机**

单步强化学习对应一个理论模型，K 臂赌博机。

每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖励，即获得最多的硬币。



- 若仅为获知每个摇臂的期望奖励，则可采用**仅探索**：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖励期望的近似估计。

- 若仅为执行奖励最大的动作，则可采用**仅利用**：按下目前最优的（即到目前为止平均奖励最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。



仅探索法能很好地估计每个摇臂的奖励，却会失去很多选择最优摇臂的机会；

仅利用法则相反，没有很好地估计摇臂期望奖励，很可能经常选不到最优摇臂。

因此，这两种方法都难以使最终的累积奖励最大化。



事实上，探索（估计摇臂的优劣）和利用（选择当前最优摇臂) 这两者是矛盾的，因为尝试次数（总投币数）有限，加强了一方则自然会削弱另一方。

这就是强化学习所面临的**探索-利用窘境**。显然，想要累积奖励最大，则必须在探索与利用之间达成较好的折中。



## Lecture 2：马尔可夫决策过程

agent 得到环境的状态后，它会采取动作，并把这个采取的动作返还给环境。环境得到智能体的动作后，它会进入下一个状态，把下一个状态传给智能体。在 RL 中，agent 与环境就是这样进行交互的，这个交互过程可以通过马尔可夫决策过程来表示。

**马尔可夫决策过程是强化学习的基本框架.**



### 2.1 马尔可夫性质

![截屏2025-01-28 00.38.37](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 00.38.37.png)

一个状态的下一个状态，取决于当前状态，而与之前的状态无关。

对于 $h_t={s_1,s_2....}$ 包含了所有的状态，但是当前的转移从当前 $s_t$ 转到 $s_{t+1}$ 直接就等于之前所有的状态。

**当我们描述一个过程满足马尔可夫特征的时候，就是在说未来的转移跟过去是独立的，只取决于现在。**

马尔可夫性质是所有马尔可夫过程的基础。



#### 马尔可夫性质的具体定义

假设有一个随机过程 $X_t$，其中 $t$ 表示时间（可以是离散的或连续的），随机变量 $X_t $ 代表时间 $t$ 时的状态。

如果满足以下条件：

$$
P(X_{t+1} \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} \mid X_t),
$$
即：给定当前状态 $X_t$ ，未来的状态 $X_{t+1}$ 与过去的状态 $X_{t-1}, \dots, X_0$ 无关，那么这个随机过程就是一个马尔可夫过程。

简单来说，马尔可夫性指的是当前状态完全决定未来状态，过去的信息可以被忽略。

> 具有马尔可夫性质并不是意味着这个随机过程就和历史完全没有关系了。
>
> 虽然 $t+1$ 时刻的状态只与 $t$ 状态有关，但是 $t$ 时刻的状态其实是包含了 $t-1$ 时刻状态的信息。
>
> 是通过这种链式的关系，历史的信息被传递到了现在。
>
> 马尔可夫性质简化了运算，因为只要当前的状态是可知的，所有历史信息就不重要了，利用当前状态信息就可以决定未来。



### 2.2 马尔可夫链

![截屏2025-01-28 00.49.23](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 00.49.23.png)

离散时间的马尔可夫过程也称为**马尔可夫链（Markov chain）**。

马尔可夫链是最简单的马尔可夫过程，其状态是有限的。

上图有 4 个状态，这 4 个状态在 *s*1*, s*2*, s*3*, s*4 之间互相转移。

从 *s*1 开始，*s*1 有 0.1 的概率继续存留在 *s*1 状态，有 0.2 的概率转移到 *s*2，有 0.7 的概率转移到 *s*4 。



可以使用状态转移矩阵来描述状态转移 $p(s_{t+1}=s'|s_t=s)$
$$
P = 
\begin{bmatrix}
P(s_1|s_1) & P(s_2|s_1) & \cdots & P(s_N|s_1) \\
P(s_1|s_2) & P(s_2|s_2) & \cdots & P(s_N|s_2) \\
\vdots     & \vdots     & \ddots & \vdots     \\
P(s_1|s_N) & P(s_2|s_N) & \cdots & P(s_N|s_N)
\end{bmatrix}
$$
这个状态转移矩阵 P 的作用类似于条件概率，表示我们知道当前在状态 $s_t$ 时，到达下面所有状态的概率。

$P$ 的第一行表示从 $s_1$ 转移到不同状态的概率



### 2.3 马尔可夫奖励过程

![截屏2025-01-28 01.02.18](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.02.18.png)

**马尔可夫奖励过程（Markov reward process, MRP）**是马尔可夫链加上奖励函数。

在马尔可夫奖励过程中，状态转移矩阵和状态都与马尔可夫链一样，只是多了奖励函数。

奖励函数 $R$ 是一个期望，表示当我们到达某一个状态的时候，可以获得多大的奖励。

另外定义了折扣因子 $γ$。如果状态数是有限的，那么 $R$ 可以是一个向量。



![截屏2025-01-28 01.05.07](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.05.07.png)

范围（horizon）是指一个回合的长度（每个回合最大的时间步数），它是由有限个步数决定的。

回报（return）可以定义为奖励的逐步叠加，假设时刻 $ t $ 后的奖励序列为 $ r_{t+1}, r_{t+2}, r_{t+3}, \cdots $，则回报为 
$$
\begin{equation} G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \gamma^3 r_{t+4} + \cdots + \gamma^{T-t-1} r_T \end{equation}
$$
 其中，$ T $ 是最终时刻，$ \gamma $ 是折扣因子，越往后得到的奖励，折扣越多。这表示更希望得到现有的奖励，对未来的奖励要打折扣。



#### 2.3.1 价值函数

当有了回报之后，就可以定义状态的价值了，就是价值函数（value function）。

**对于马尔可夫奖励过程，价值函数被定义成回报的期望，即 **
$$
V^t(s) = \mathbb{E}[G_t \mid s_t = s] = \mathbb{E}[r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots + \gamma^{T-t-1} r_T \mid s_t = s]
$$
其中，$ G_t $ 是之前定义的折扣回报。

对 $ G_t $ 取了一个期望，期望就是从这个状态开始，可能获得多大的价值。

所以期望也可以看成未来可能获得奖励的当前价值的表现，就是当我们进入某一个状态后，我们现在有多大的价值



**贝尔曼方程：**

![截屏2025-01-28 01.24.03](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.24.03.png)

从价值函数里面推导出**贝尔曼方程（Bellman equation）**: 
$$
\begin{equation} V(s) = \underbrace{R(s)}_{\text{即时奖励}} + \gamma \underbrace{\sum_{s' \in S} p(s' \mid s) V(s')}_{\text{未来奖励的折扣总和}} \end{equation}
$$
其中，$ s' $ 可以看成未来的某个状态，$ p(s' \mid s) $ 是指从当前状态转移到未来状态的概率。

这个式子的推导如下：
$$
\begin{align*}
V(s) &= \mathbb{E}[G_t | S_t = s] \\
     &= \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots | S_t = s] \\
     &= \mathbb{E}[R_t + \gamma (R_{t+1} + \gamma R_{t+2} + \ldots) | S_t = s] \\
     &= \mathbb{E}[R_t + \gamma G_{t+1} | S_t = s] \\
     &= \mathbb{E}[R_t + \gamma V(S_{t+1}) | S_t = s]
\end{align*}
$$
其中，即时奖励的期望就是当前奖励函数的输出：
$$
\mathbb{E}[R_t\mid S_t=s]=R(s)
$$
另一方面，剩余部分可以根据状态 $s$ 除法的转移概率得到，即：
$$
\mathbb{E}[\gamma V(S_{t+1}) | S_t = s]=\gamma{\sum_{s' \in S} p(s' \mid s) V(s')}
$$


贝尔曼方程中，$ V(s') $ 代表的是未来某一个状态的价值。

从当前状态开始，有一定的概率去到未来的所有状态，所以要把 $ p(s' \mid s) $ 写上去。我们、、得到了未来状态后，乘一个 $ \gamma $​，把未来的奖励打折扣。

$ \gamma \sum_{s' \in S} p(s' \mid s) V(s') $​ 可以看成未来奖励的折扣总和。 

**贝尔曼方程定义了当前状态与未来状态之间的关系。未来奖励的折扣总和加上即时奖励，就组成了贝尔曼方程。**



状态转移概率乘它未来的状态的价值，再加上它的即时奖励，就会得到它当前状态的价值。

贝尔曼方程定义的就是当前状态与未来状态的迭代关系。
$$
\begin{pmatrix} V(s_1) \\ V(s_2) \\ \vdots \\ V(s_N) \end{pmatrix} = \begin{pmatrix} R(s_1) \\ R(s_2) \\ \vdots \\ R(s_N) \end{pmatrix} + \gamma \begin{pmatrix} p(s_1 \mid s_1) & p(s_2 \mid s_1) & \cdots & p(s_N \mid s_1) \\ p(s_1 \mid s_2) & p(s_2 \mid s_2) & \cdots & p(s_N \mid s_2) \\ \vdots & \vdots & \ddots & \vdots \\ p(s_1 \mid s_N) & p(s_2 \mid s_N) & \cdots & p(s_N \mid s_N) \end{pmatrix} \begin{pmatrix} V(s_1) \\ V(s_2) \\ \vdots \\ V(s_N) \end{pmatrix}
$$


所有状态价值是向量 $[V(s_1), V(s_2), \cdots, V(s_N)]^\mathsf{T}$。

每一行来看，向量 $\mathbf{V}$ 乘状态转移矩阵里面的某一行，再加上它当前可以得到的奖励，就会得到它当前的价值。 

当我们把贝尔曼方程写成矩阵形式后，可以直接求解：
$$
\mathbf{V} = \mathbf{R} + \gamma \mathbf{P} \mathbf{V} \\
I \mathbf{V} = \mathbf{R} + \gamma \mathbf{P} \mathbf{V} \\
(I - \gamma \mathbf{P}) \mathbf{V} = \mathbf{R} \\
\mathbf{V} = (I - \gamma \mathbf{P})^{-1} \mathbf{R} \\
$$
 可以直接得到解析解： 
$$
\begin{equation} \mathbf{V} = (I - \gamma \mathbf{P})^{-1} \mathbf{R} \tag{2.15} \end{equation} 
$$


可以通过矩阵求逆把 $\mathbf{V}$ 的值直接求出来。但是问题是这个矩阵求逆的过程的复杂度是 $O(N^3)$。

当状态非常多的时候，比如从 10 个状态到 1000 个状态，或者到 100 万个状态，当我们有 100 万个状态的时候，状态转移矩阵就会是一个 100 万乘 100 万的矩阵，对这样一个大矩阵求逆是非常困难的。

所以这种通过解析解去求解的方法只适用于很小量的马尔可夫奖励过程。 



求解比较大规模的马尔可夫奖励过程中的价值函数，可以使用

1. 蒙特卡洛
2. 动态规划
3. 时序差分



#### 2.3.2 为什么需要折扣因子 $\gamma$

![截屏2025-01-28 01.10.05](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.10.05.png)



- 有些马尔可夫过程是带环的，它并不会终结，避免无穷的奖励。
- 我们并不能建立完美的模拟环境的模型，对未来的评估不一定是准确的，不一定完全信任模型，因为这种不确定性，所以我们对未来的评估增加一个折扣。我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励。
- 如果奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）。
- 最后，我们也更想得到即时奖励。有些时候可以把折扣因子设为 0 ($ \gamma = 0 $)，我们就只关注当前的奖励。我
  们也可以把折扣因子设为 1 ($ \gamma = 1 $)，对未来的奖励并没有打折扣，未来获得奖励与当前获得的奖励是一样的



### 2.4 马尔可夫决策过程

![截屏2025-01-28 01.37.17](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.37.17.png)

相对于马尔可夫奖励过程，马尔可夫决策过程多了决策（动作），其他的定义与马尔可夫奖励过程的是类似的。

此外，状态转移也多了一个条件，变成了 $ p(s_{t+1} = s' \mid s_t = s, a_t = a) $。

**未来的状态不仅依赖于当前的状态，也依赖于在当前状态智能体采取的动作。**

马尔可夫决策过程满足条件：
$$
\begin{equation} p(s_{t+1} \mid s_t, a_t) = p(s_{t+1} \mid h_t, a_t)\end{equation}
$$
 对于奖励函数，它也多了一个当前的动作，变成了 $ R(s_t = s, a_t = a) = \mathbb{E}[r_t \mid s_t = s, a_t = a] $。

当前的状态以及采取的动作会决定智能体在当前可能得到的奖励多少。



#### 2.4.1 策略 in MDP

策略定义了在某个状态 $s$ 可以采取什么样的动作，知道当前状态以后，可以把当前状态代入策略函数来得到一个概率，即
$$
\pi(a|s)=p(a_t=a|s_t=s)
$$


这表示在输入状态 $s$ 情况下，采取动作 $a$ 的概率。

概率代表在所有可能的动作里面怎样采取行动，比如可能有 0.7 的概率往左走，有 0.3 的概率往右走。

这种方式叫做**随机性策略**，对于随机性策略，每个状态输出的是关于动作的概率分布，然后根据这个分布进行采样得到一个动作。

还有一种**确定性策略**，直接确定输出某个动作。



**策略作用下的马尔可夫决策过程**

在马尔可夫决策过程中，也可以定义类似的价值函数。

此时的价值函数与策略有关，这表示对于两个不同的策略来说，在同一个状态下的价值很可能也是不同的。



已知马尔可夫决策过程和策略 $\pi$​，可以把马尔可夫决策过程转换成马尔可夫奖励过程。

在马尔可夫决策过程中，状态的转移依赖于**当前的状态 $s$ 和动作 $a$**，状态转移概率为：$p(s’ | s, a)$

即：在状态 $s$，采取动作 $a$ 后转移到状态 $s’$ 的概率。



#### 2.4.2 状态价值函数 $V_{\pi}(s)$

马尔可夫决策过程中的价值函数可定义为：

$$
V_{\pi}(s) = \mathbb{E}_{\pi} \left[ G_t \mid s_t = s \right]
$$
定义为从状态 $s$ 出发遵循策略 $\pi$ 能得到的期望回报。

当策略决定后，通过对策略进行采样来得到一个期望，计算出它的价值函数。



#### 2.4.3 动作价值函数 $Q^{\pi}(s,a)$

在 MDP 中，由于动作的存在，需要额外定义一个动作价值函数 $Q$。

**$Q$ 函数表示在状态 $s$ 下执行动作 $a$ 后，能获得的期望累积回报**，即
$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ G_t \mid s_t = s, a_t = a \right]
$$


**状态价值函数 $V$ 和动作价值函数 $Q$ 之间的关系：**

在使用策略 $\pi$ 中，**状态的价值 = 在该状态下基于策略采取所有动作的概率与相应的价值相乘再求和的结果：**
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) Q_{\pi}(s, a)
$$

> 在策略 $\pi$ 下，状态 $s$ 的值函数 $V_{\pi}(s)$ 是**所有可能动作值函数 $Q_{\pi}(s,a)$的加权平均**，权重为策略 $\pi$ 在状态 $s$ 下选择动作 $a$ 的概率 $\pi(a \mid s)$。



#### 2.4.3 状态价值函数 $V$ 和动作价值函数 $Q$ 的关系推导过程：

状态价值函数的定义为：
$$
V_{\pi}(s) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \mid s_t = s \right]
$$
这表示在状态  $s$  下，按照策略 $\pi$ 采取行动后，所能获得的预期总回报。

状态-动作价值函数定义为：
$$
Q_{\pi}(s, a) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \mid s_t = s, a_t = a \right]
$$
这表示在状态 $$ s $$ 下执行特定动作 $$ a $$，然后按照策略 $$ \pi $$ 继续行动，所能获得的预期总回报。



推导过程：
$$
V_{\pi}(s) = \mathbb{E}_\pi [ G_t \mid s_t = s ]
$$
由于策略 $$ \pi(a \mid s) $$ 给出了在状态 $$ s $$ 下选择动作 $$ a $$ 的概率，因此可以对所有可能的动作进行期望计算：
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) \mathbb{E}_\pi \left[ G_t \mid s_t = s, a_t = a \right]
$$
根据 $$ Q_{\pi}(s, a) $$ 的定义：
$$
Q_{\pi}(s, a) = \mathbb{E}_\pi \left[ G_t \mid s_t = s, a_t = a \right]
$$
代入得：
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) Q_{\pi}(s, a)
$$

**一个状态的价值等于该状态下所有可能动作的价值 $Q$ 的期望值，其中期望是按照当前策略 $$ \pi $$ 计算的**。



#### 2.4.4 对 $Q_{\pi}(s,a)$​对函数的贝尔曼方程进行推导：

$Q$ 函数的定义为：
$$
Q(s, a) = \mathbb{E}\left[ G_t \mid s_t = s, a_t = a \right]
$$
$$ Q(s, a) $$ 表示在状态 $$ s $$ 采取动作 $$ a $$ 后，未来能获得的期望累计回报。
$$
Q(s, a) = \mathbb{E}\left[ r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots \mid s_t = s, a_t = a \right]
$$

$$
Q(s, a) = \mathbb{E}\left[ r_{t+1} \mid s_t = s, a_t = a \right] + \gamma \mathbb{E}\left[ r_{t+2} + \gamma r_{t+3} + \gamma^2 r_{t+4} + \cdots \mid s_t = s, a_t = a \right]
$$

$$
Q(s, a) = R(s, a) + \gamma \mathbb{E}\left[ G_{t+1} \mid s_t = s, a_t = a \right]
$$

其中，$$ R(s, a) $$ 是奖励函数：$$R(s, a) = \mathbb{E} \left[ r_{t+1} \mid s_t = s, a_t = a \right]$$，即，在状态 $$ s $$ 采取动作 $$ a $$​ 后，所能获得的期望奖励。

而在马尔可夫决策过程中，$V$ 函数的定义是 $V(s_{t+1}) = \mathbb{E} \left[ G_{t+1} \mid s_{t+1} \right]$，所以：
$$
Q(s, a) = R(s, a) + \gamma \mathbb{E}\left[ V(s_{t+1}) \mid s_t = s, a_t = a \right]
$$
由于是对 $V(s')$ 求期望，所以需要对所有可能的 $s'$ 进行加权求和，权重是状态转移概率：
$$
Q(s, a) = R(s, a) + \gamma \sum_{s’ \in S} p(s’ \mid s, a) V(s’)
$$
**$Q$ 函数的贝尔曼方程：**
$$
Q(s,a)=R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V(s')
$$
其中前半部分 $R(s,a)$ 表示在状态 $s$ 下执行动作 $a$ 后，环境直接反馈的奖励。

而后半部分 $\gamma \sum_{s' \in S} p(s' \mid s, a) V(s')$ 表示未来奖励的期望值。



**$V$ 函数评估的是当前 $s$ 的整体好坏，比如离终点越近， $V$ 值越高**

**$Q$函数告诉在这个 $S$ 向某个方向走的具体价值，帮助选择最佳的路径**



#### 2.4.5 值函数 $V$ 的贝尔曼期望方程

对状态价值函数 $V^{\pi}$ 进行分解，可以得到一个类似于之前马尔可夫奖励过程的贝尔曼方程——贝尔曼期望方程
$$
V_{\pi}(s) = \mathbb{E}_{\pi} \left[ r_{t+1} + \gamma V_{\pi}(s_{t+1}) \mid s_t = s \right]
$$
对 $Q$ 函数也做类似的分解，得到 $Q$ 函数的贝尔曼期望方程
$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ r_{t+1} + \gamma Q_{\pi}(s_{t+1}, a_{t+1}) \mid s_t = s, a_t = a \right]
$$
 **贝尔曼期望方程定义了当前状态和未来状态的关联。**


$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) Q_{\pi}(s, a)
$$

$$
Q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V_{\pi}(s')
$$

(38)和(39)代表状态价值函数与 Q 函数之间的关联。



把(39)代入(38)可以得到：
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) \left( R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V_{\pi}(s') \right)
$$
表示当前状态的价值与未来状态价值之间的关联，描述了在策略 $$ \pi $$ 下，状态 $$ s $$ 的值函数 $$ V_{\pi}(s) $$ 是如何递归定义的

> 该方程的意义是：**状态 $$ s $$ 的期望回报等于在该状态下所有可能的动作的加权平均回报**，其中：
>
> **策略 $$ \pi(a \mid s) $$**：表示在状态 $$ s $$ 下采取动作 $$ a $$ 的概率。
>
> **立即奖励 $$ R(s, a) $$**：在状态 $$ s $$ 采取动作 $$ a $$ 所获得的期望奖励：
>
> **未来回报 $$ \sum_{s’ \in S} p(s’ \mid s, a) V_{\pi}(s’) $$**：
>
> $$ p(s’ \mid s, a) $$ 表示从 $$ s $$ 采取 $$ a $$ 之后转移到 $$ s’ $$ 的概率。
>
> $$ V_{\pi}(s’) $$ 表示从 $$ s’ $$ 开始的期望回报。
>
> 表示所有可能的下一状态 $$ s’ $$ 的值函数 $$ V_{\pi}(s’) $$ 的加权和。



**举例说明**

假设在某个状态 $$ s $$：可以选择 $$ a_1 $$ 或 $$ a_2 $$ 两个动作。策略 $$ \pi $$ 规定：以 70% 概率选择 $$ a_1 $$，30% 概率选择 $$ a_2 $$。

立即奖励：$$ R(s, a_1) = 5 $$，$$ R(s, a_2) = 2 $$。

状态转移概率：

- $ a_1 $ 导致 80% 的概率转移到 $$ s_1 $$（$ V(s_1) = 10 $），20% 的概率转移到 $$ s_2 $$（$$ V(s_2) = 5 $$）。

- $$ a_2 $$ 导致 50% 的概率转移到 $$ s_1 $$，50% 的概率转移到 $$ s_2 $$。

假设折扣因子 $$ \gamma = 0.9 $$​。
$$
V_{\pi}(s) = 0.7 \left( 5 + 0.9(0.8 \times 10 + 0.2 \times 5) \right) + 0.3 \left( 2 + 0.9(0.5 \times 10 + 0.5 \times 5) \right)=11.48
$$
这个值表示的意义是：**如果从 $$ s $$ 开始，按照策略 $$ \pi $$ 采取动作，长期回报的期望值是 11.48**。



值函数 $V$ 的贝尔曼期望方程是，**当前状态的值是未来所有可能路径的加权总和**。通过递归关系，可以计算出所有状态的值，从而评估不同策略的好坏。

在 RL 的策略评估中，通常可以利用这个方程迭代计算 状态值函数 $$ V_{\pi}(s) $$，然后基于它进行策略改进，从而求解最优策略。



#### 2.4.6 $Q$ 函数的贝尔曼期望方程

把(38)代入(39)可以得到
$$
Q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) \sum_{a' \in A} \pi(a' \mid s') Q_{\pi}(s', a')
$$
这表示当前时刻的 $Q$ 函数与未来时刻的 $Q$ 函数之间的关联

这个式子体现了 RL 中的递归思想：当前的 $Q$ 值 = 立即奖励 + 未来 $Q$ 值的折扣折扣加权和。

未来的回报取决于所有可能的下一个状态 $s{\prime}$ 以及在 $s{\prime}$ 处可能采取的所有动作 $a{\prime}$。



#### 2.4.7 将 MDP 转化为 MRP

已知策略 $\pi$​​ 后，可以通过**将动作的影响加权平均**，将马尔可夫决策过程转换为马尔可夫奖励过程。

> 确定策略以后，所有动作的概率进行加权，得到的奖励和可以认为是一个 MRP 在该状态 $s$ 下的奖励。

这涉及两个部分：**状态转移函数**和**奖励函数**。



**(1) 状态转移函数**

原本的状态转移函数依赖于动作 $a$：$$p(s’ | s, a)$$。

已知策略 $\pi(a|s)$ 后，可以对动作 $a$ 加权求和，得到**不依赖于动作的状态转移函数**：
$$
P_{\pi}(s’|s) = \sum_{a \in A} \pi(a|s)~ p(s’|s, a)
$$
这个式子表达的意思是：基于策略 $\pi$，计算从状态 $s$ 转移到状态 $s’$ 的概率，考虑了所有可能的动作及其概率。



**(2) 奖励函数**

原本的奖励函数依赖于动作 $a$：$R(s, a)$。

同样通过策略 $\pi(a|s)$ 对动作 $a$ 加权求和，得到**不依赖于动作的奖励函数**：
$$
r_{\pi}(s) = \sum_{a \in A} \pi(a|s) R(s, a)
$$
这个式子表达的意思是在状态 $s$ 中，**按照策略 $\pi$ 选择动作时，所有可能动作的即时奖励的期望值**。
即，根据策略 $\pi$ 的动作概率分布，对每个动作的即时奖励进行加权求和。



#### 2.4.8 MDP 和 MRP 的区别

![截屏2025-01-28 11.45.54](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 11.45.54.png)

**MRP**

MRP 是一种简化的随机过程，不含动作，只有状态转移和奖励。MRP 是 MDP 在给定策略 $\pi$ 后的简化形式。

马尔可夫奖励过程的状态转移是直接决定的。

比如当前状态是 *s*，那么直接通过转移概率决定下一个状态是什么。



**MDP**

MDP 是一个包含动作选择的随机过程，用于描述 agent 在环境中通过采取动作、与环境交互以获得奖励的过程

但对于马尔可夫决策过程，它的中间多了一层动作 $a$ ，即 agent 在当前状态的时候，首先要决定采取某一种动作，到达某一个黑色的节点。到黑色的节点后，因为有一定的不确定性，所以当 agent 当前状态以及智能体当前采取的动作决定过后，agent 进入未来的状态其实也是一个概率分布。

在当前状态与未来状态转移过程中多了一层决策性，这是马尔可夫决策过程与之前的马尔可夫奖励过程很不同的一点。在马尔可夫决策过程中，动作是由 agent 决定的，agent 会采取动作来决定未来的状态转移



**备份图(回溯图)**

在 RL 中，**备份图（Backup Diagram）** 是一种可视化工具，描述价值函数（如 $V(s)$ 或 $Q(s,a)$）的更新过程。

通过图形化的方式展示**当前状态（或状态-动作对）与后续状态（或动作）之间的依赖关系**，帮助理解算法如何将未来信息“回溯”到当前状态的更新中。

![截屏2025-01-28 13.57.19](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 13.57.19.png)

上图有两层加和。第一层加和是对叶子节点进行加和，往上备份一层，我们就可以把未来的价值（$s'$ 的价值）备份到黑色的节点。

第二层加和是对动作进行加和，得到黑色节点的价值后，往上备份一层，得到根节点的价值，即当前状态的价值。



对于这个图的上半部分，从 $a$ 到 $v_{\pi}(s)$，给出了状态价值函数 $Q$ 和 $V$ 函数的关系
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) Q_{\pi}(s, a)
$$
下半部分，计算 $Q$ 函数为：
$$
Q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V_{\pi}(s')
$$
把下面这个式子代入上面这个 $V$ 可以得到：
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) \left( R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V_{\pi}(s') \right)
$$
所以备份图定义了未来下一时刻的状态价值函数与上一时刻的状态价值函数之间的关联。



![截屏2025-01-28 14.03.46](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 14.03.46.png)

对于 $Q$​ 函数同理，根节点是 Q 函数的一个节点。Q 函数对应于黑色的节点。下一时刻的 Q 函数对应于叶子节点，有 4 个黑色的叶子节点。

这里也有两层加和。第一层加和先把叶子节点从黑色节点推到空心圆圈节点，进入到空心圆圈结点的状态。

当到达某一个状态后，再对空心圆圈节点进行加和，这样就把空心圆圈节点重新推回到当前时刻的 Q 函数。
$$
V_{\pi}(s') = \sum_{a' \in A} \pi(a' \mid s') Q_{\pi}(s', a')
$$

$$
Q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) \sum_{a' \in A} \pi(a' \mid s') Q_{\pi}(s', a')
$$

**策略评估**

策略评估是给定 马尔可夫决策过程和策略，评估可以获得多少价值，即对于当前策略，可以得到多大的价值。

可以直接把**贝尔曼期望备份（Bellman expectation backup）** ，变成迭代的过程，反复迭代直到收敛。

这个迭代过程可以看作**同步备份（synchronous backup）** 的过程。
$$
V^{t+1}(s) = \sum_{a \in A} \pi(a \mid s) \left( R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V^t(s') \right)
$$
当得到上一时刻的 $V_t$ 的时候，就可以通过递推的关系推出下一时刻的值。

反复迭代，最后 $V$ 的值就是从 $V_1,V_2$ 到最后收敛之后的值 $V_{\pi}$。

$V_{\pi}$就是当前给定的策略 $\pi$ 对应的价值函数。



**马尔可夫决策过程控制**

策略评估是指给定马尔可夫决策过程和策略，我们可以估算出价值函数的值。

若只有马尔可夫决策过程，该如何寻找最佳的策略，从而得到最佳价值函数？ 

最佳价值函数的定义为
$$
\begin{equation} V^*(s) = \max_{\pi} V_{\pi}(s) \end{equation}
$$
最佳价值函数是指，搜索一种策略 $\pi$ 让每个状态的价值最大。

$V^*$ 就是到达每一个状态，它的值的最大化情况。在这种最大化情况中，得到的策略就是最佳策略，即 
$$
\begin{equation} \pi^*(s) = \arg\max_{\pi} V_{\pi}(s) \end{equation}
$$
最佳策略使得每个状态的价值函数都取得最大值。

所以如果可以得到一个最佳价值函数，就可以认为某个马尔可夫决策过程的环境可解。

在这种情况下，最佳价值函数是一致的，环境中可达到的上限的值是一致的，但这可能存在多个最佳策略，多个最佳策略可以取得相同的最佳价值。 ![截屏2025-01-28 17.01.31](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 17.01.31.png)

当取得最佳价值函数后，我们可以通过对 $Q$ 函数进行最大化来得到最佳策略： 
$$
\begin{equation} \pi^*(a \mid s) =  \begin{cases}  1, & a = \arg\max_{a \in A} Q^*(s, a) \\ 0, & \text{其他} \end{cases} \end{equation}
$$
当 $Q$ 函数收敛后，因为 $Q$​ 函数是关于状态与动作的函数，所以如果在某个状态采取 某个动作，可以使得 $Q$ 函数最大化，那么这个动作就是最佳的动作。

如果能优化出一个 Q 函数 $Q^*(s,a)$​，就可以直接在 $Q$ 函数中取一个让 $Q$ 函数值最大化的动作的值，就可以提取出最佳策略。



搜索最佳策略可以采用穷举，但是效率太低，一般可以采用**策略迭代**和**价值迭代**。



### 2.5 蒙特卡洛



### 2.6 策略迭代(Policy Iteration)



![截屏2025-01-29 14.08.48](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-29 14.08.48.png)

可以把 $Q$ 函数看成一个 $Q$ 表格：横轴是它的所有状态，纵轴是它的可能的动作。

当得到 $Q$ 函数，$Q$ 表格也就得到了。对于某个状态，每一列取最大的值，最大值对应的动作就是它现在应该采取的动作。

**$\arg\max$ 操作是指在每个状态里面采取一个动作，这个动作是能使这一列的 $Q$ 函数值最大化的动作**



**贝尔曼最优方程**

通过对上面的 $Q$ 函数取贪心的操作，会得到更好或不变的策略。当停止改进的时候，会的得到一个最佳策略，取让 $Q$ 函数值最大化的动作， $Q$ 函数就会直接变成价值函数，即：
$$
\begin{equation} Q_{\pi}(s, \pi'(s)) = \max_{a \in A} Q_{\pi}(s, a) = Q_{\pi}(s, \pi(s)) = V_{\pi}(s) \end{equation} 
$$
我们也就可以得到贝尔曼最优方程
$$
 \begin{equation} V_{\pi}(s) = \max_{a \in A} Q_{\pi}(s, a) \end{equation} 
$$
贝尔曼最优方程表明：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的回报的期望。

当马尔可夫决策过程满足贝尔曼最优方程的时候，整个马尔可夫决策过程已经达到最佳的状态。

只有当整个状态已经收敛后，我们得到最佳价值函数后，贝尔曼最优方程才会满足。

满足贝尔曼最优方程后，我们可以采用最大化操作，即
$$
\begin{equation} V^*(s) = \max_{a} Q^*(s, a) \end{equation} 
$$
取让 $Q$ 函数值最大化的动作对应的值就是当前状态的最佳的价值函数的值。 



**$Q$ 函数的贝尔曼方程，$Q$ 函数之间的转移**
$$
\begin{align*} Q^*(s, a) &= R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V^*(s') \\ &= R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) \max_{a'} Q^*(s', a') \end{align*} 
$$
RL 中的 $Q$ 学习是基于贝尔曼最优方程来进行的，当取 $Q$ 函数值最大的状态（$\max_{a'} Q^*(s', a')$）的时候可得
$$
 \begin{equation} Q^*(s, a) = R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) \max_{a'} Q^*(s', a') \end{equation} 
$$
**状态价值函数的转移：**
$$
\begin{align*} V^*(s)  &= \max_{a} \left( R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V^*(s') \right) \end{align*}
$$

> 取 max 操作选择当前最优动作，确保每一步决策都可以最大化长期的奖励。







### 2.10 计算马尔可夫奖励过程价值

![截屏2025-01-28 01.34.13](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.34.13.png)



**1.蒙特卡洛**

![截屏2025-01-28 01.34.46](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.34.46.png)



**2.动态规划**

![截屏2025-01-28 01.36.38](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.36.38.png)







## Chap3.动态规划算法

基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。

动态规划会保存已解决的子问题的答案，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。



基于动态规划的强化学习算法主要有两种：一是**策略迭代**（policy iteration），二是**价值迭代**（value iteration）

其中，策略迭代由两部分组成：**策略评估**（policy evaluation）和**策略提升**（policy improvement）。



策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程；

而价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。



### 3.1 策略迭代

策略迭代由两个步骤组成：策略评估和策略改进。

**第一个步骤是策略评估**，当前我们在优化策略 $\pi$，在优化过程中得到一个最新的策略。我们先保证这个策略不变，然后估计它的价值，即给定当前的策略函数来估计状态价值函数。

**第二个步骤是策略改进**，得到状态价值函数后，我们可以进一步推算出它的 $Q$ 函数。得到 $Q$ 函数后，直接对 $Q$ 函数进行最大化，通过在 $Q$ 函数做一个贪心的搜索来进一步改进策略。这两个步骤一直在迭代进行。



#### 3.1.1 策略评估



**贝尔曼期望方程**
$$
V^{\pi}(s) = \sum_{a \in A} \pi(a \mid s) \left( r(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V^{\pi}(s') \right)
$$


**知道奖励函数 $r(s,a)$ 和状态转移函数时，就可以根据下一个状态的价值来计算当前状态的价值。**

这符合动态规划的思想，把下一个可能的状态的价值当做一个子问题，把计算当前状态的价值看作是当前的问题。

知道子问题的解以后，可以利用子问题的解求解当前的问题。



可以从任意一个初始的 $V_0(s)$ 开始，不断的应用贝尔曼期望方程进行更新，直到收敛

在实际操作中，可以设定一个阈值 $\theta$，当所有状态价值变化量小于 $\theta$ 时，则认为算法收敛了。

一般可以设置 $\theta=1e-6$ ，确保价值基本稳定。



**进行策略评估的目的是计算出 $\pi$ 下每个状态的价值函数，用于评估策略的好坏，决定是否要改进策略。**



#### 3.1.2 策略提升



通过上一步的策略评估，已经知道价值 $V^{\pi}$ 。

假设 agent 在状态 $s$ 下采取动作 $a$，之后的动作仍然遵循 $\pi$，得到的期望回报就是动作价值 $Q^{\pi}(s,a)$。

若 $Q^{\pi}(s,a)$ 大于 $V^{\pi}(s)$，这意味着策略 $\pi$ 可能不是最优的，至少在状态 $s$ 下可以改进策略，让 agent 直接选择 $a$ 来提高整体回报。



这个假设只是针对一个状态，假设存在一个确定性策略 $\pi'$ 在任意的 $s$ 下，都满足 $Q^{\pi}(s,\pi'(s))>V^{\pi}(s)$

所以在任意状态 $s$ 下，都有 $V^{\pi'}(s)\geq V^{\pi}(s)$，这就是**策略提升定理**。



策略提升定理是 RL 中策略迭代的理论基础，说明了**如果一个策略在所有状态下选择的动作都比当前策略的动作价值更优，则新策略的回报一定不低于旧策略**。

策略提升定义说明了，新策略 $\pi'$ 至少会和旧的策略 $\pi$ 一样好，至少不会变差，采用新策略 $\pi'$ 的期望回报一定不会小于旧策略 $\pi$ 下的期望回报。



有了策略提升定理，就可以贪心地在每一个状态 都选择当前的动作价值最大的动作，用公式表示就是：
$$
\pi'(s) = \arg\max_{a} Q^{\pi}(s, a) = \arg\max_{a} \{ r(s,a) + \gamma \sum_{s'} P(s'|s,a) V^{\pi}(s') \}
$$
根据贪心策略 选择动作从而得到新策略的过程称为策略提升。

当策略提升之后得到的策略 $\pi'$ 和之前的策略 $\pi$ 一样时，说明策略迭代达到了收敛，此时 $\pi$ 和 $\pi'$ 就是最优策略。



#### 3.1.3 策略提升定理的证明

策略提升定理的前提是：$V^{\pi}(s)\leq Q^{\pi}(s,\pi'(s))$

当
$$
V^{\pi}(s) \leq Q^{\pi}(s, \pi'(s))=
\mathbb{E}_{\pi'} [R_t + \gamma V^{\pi}(S_{t+1}) | S_t = s]
$$
所以
$$
V^{\pi}(s) \leq Q^{\pi}(s, \pi'(s))
\leq \mathbb{E}_{\pi'} [R_t + \gamma Q^{\pi}(S_{t+1}, \pi'(S_{t+1})) | S_t = s]
$$
则：
$$
V^{\pi}(s) \leq Q^{\pi}(s, \pi'(s))
\leq \mathbb{E}_{\pi'} [R_t + \gamma Q^{\pi}(S_{t+1}, \pi'(S_{t+1})) | S_t = s]

\\
= \mathbb{E}_{\pi'} [R_t + \gamma R_{t+1} + \gamma^2 V^{\pi}(S_{t+2}) | S_t = s]


\\
\leq \mathbb{E}_{\pi'} [R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 V^{\pi}(S_{t+3}) | S_t = s]

\\
\vdots
\\
\leq \mathbb{E}_{\pi'} [R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots | S_t = s]
\\
= V^{\pi'}(s)
$$
每一个时间步都利用到局部动作价值优先 $V^{\pi}(S_{t+1}) \leq Q^{\pi}(S_{t+1}, \pi'(S_{t+1}))$。



#### 3.1.4 策略迭代算法

评估-->提升-->评估-->提升-->评估-->提升-->评估-->提升

![截屏2025-01-30 17.51.52](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-30 17.51.52.png)



### 3.2 价值迭代

在价值迭代中，不存在显示的策略，只是维护一个状态价值函数
$$
V^*(s) = \max_{a \in \mathcal{A}} \left\{ r(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s, a) V^*(s') \right\}
$$
其中：

$V^*(s)$：表示状态 $s$ 的最优价值

$P(s'\mid s,a)$：在状态 $s$ 采取动作 $a$ 转移到状态 $s'$ 的概率

这个公式的意思是：

**状态** $s$ **的最优价值等于在所有可能动作  $a$ 中，选择能够最大化未来收益的动作对应的期望回报。**

![截屏2025-01-30 23.11.32](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-30 23.11.32.png)



价值迭代的核心思想是：

1.**初始化**：所有状态的价值设为 0（或者任意值）。

2.**更新状态价值**：使用 **贝尔曼方程** 迭代更新：
$$
V(s) \leftarrow \max_a \sum_{s{\prime}} P(s{\prime} | s, a) \left[ R(s, a) + \gamma V(s{\prime}) \right]
$$
3.**检查收敛**：如果所有状态的价值变化（误差）小于某个阈值 ，则停止迭代。

4.**提取最优策略**：根据最终的价值函数确定最优策略：
$$
\pi^*(s) = \arg\max_a \sum_{s{\prime}} P(s{\prime} | s, a) \left[ R(s, a) + \gamma V(s{\prime}) \right]
$$
每个状态 $s$ 选择能够最大化回报的动作 $a$
