## Chap1.RL基础



![截屏2025-01-25 15.55.00](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-25 15.55.00.png)

RL 要解决的问题是：agent 怎么在复杂、不确定的环境中最大化它能获得的奖励。

在强化学习过程中，智能体与环境一直在交互。智能体在环境中获取某个状态后，它会利用该状态输出一个action ，这个动作也称为决策（decision）。

然后这个动作会在环境中被执行，环境会根据智能体采取的动作，输出下一个状态以及当前这个动作带来的奖励。agent 的目的就是尽可能多地从环境中获取奖励。



#### RL 与监督学习

监督学习的过程中，存在两个假设：

- 输入的数据应该是前后无关联的的。
- 需要告诉学习器正确的标签是什么，以至于可以通过正确的标签来修正自己的预测。

换句话说，监督学习的样本满足独立同分布。但是 RL 不同：

- 以 雅达利游戏 breakout 为例，游戏的画面作为输入，上一帧和下一帧之间有很强的联系。数据是基于时间的序列数据，不满足独立同分布。
- 在监督学习中，模型决定了输出，就会立即得到反馈，是正确还是错误。但是在 RL 中，例如下围棋，输入了此时棋盘的画面，机器确定了落子位置以后，并不能得到一个实时的反馈，只有当游戏结束了，才能知道这个动作是正确的还是错误的。换句话说就是，RL 无法得到实时的反馈。

某个时刻的动作，对游戏的输赢有没有帮助，这件事情是不清楚的，这就是 RL 中的延迟奖励问题。这使得 RL 的训练相当困难。

与监督学习不同，机器并不知道正确的 action 是什么，机器需要自己去学习哪些动作可以带来最多的奖励，只能通过不停地尝试来发现最有利的动作。



agent 学习哪些动作可以带来最多的奖励，就是一个不断试错的过程，**探索和利用** 是RL 中非常核心的问题。

**探索：**指尝试一些新的动作， 这些新的动作有可能会使我们得到更多的奖励，也有可能使我们扣掉一些 reward；

**利用：**指采取已知的可以获得最多奖励的动作，重复执行这个动作，因为这样做可以获得一定的奖励。

因此，RL 需要在探索和利用之间进行权衡，这也是在监督学习没有的情况。



#### 传统强化学习以及深度强化学习

![截屏2025-01-25 16.09.07](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-25 16.09.07.png)

对于传统的计算机视觉：

- 给定一张图片，提取它的特征，手工设计一些好的特征，比如方向梯度直方图（histogram of oriental gradient，HOG）。
- 提取这些特征后，训练一个分类器。以是SVM，然后就可以辨别这张图片是狗还是猫。

对于深度学习中的计算机视觉：特征提取以及分类在一起。

- 训练一个神经网络，既可以做特征提取，也可以做分类，可以实现端到端训练。

![截屏2025-01-25 16.11.42](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-25 16.11.42.png)

同理，神经网络 + RL = Deep RL

- 标准强化学习：标准强化学习先设计很多特征，这些特征可以描述现在整个状态。 得到这些特征后，可以通过训练一个分类网络 或者 分别训练一个价值估计函数来采取动作。
- 深度强化学习：一个端到端训练的过程。不需要设计特征，直接输入状态就可以输出动作。用一个神经网络来拟合价值函数 或 策略网络，省去设计特征这一步。



#### 序列决策：

在与环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。**历史(History)是观测、动作、奖励的序列：**
$$
H_t = o_1, a_1, r_1, \ldots, o_t, a_t, r_t
$$
Agent 在采取当前动作的时候会依赖于它之前得到的历史，所以可以把整个游戏的状态看成关于这个历史的函数：
$$
S_t = f(H_t)
$$


**状态**是对世界的完整描述，不会隐藏世界的信息。

**观测**是对状态的部分描述，可能会遗漏一些信息。

在 DRL 中，用向量、矩阵或者更高阶的张量来表示状态和观测。



#### MDP与 POMDP 的概念

**马尔可夫决策过程**

环境有自己的函数 $ s_t^e = f^e(H_t) $ 来更新状态，在智能体的内部也有一个函数 $ s_t^a = f^a(H_t) $ 来更新状态。

当 agent 的状态与环境的状态等价时，即 agent 能够观察到环境的所有状态时，就称这个环境是完全可观测的。

在这种情况下，强化学习通常被建模成一个 **马尔可夫决策过程** 的问题。

在马尔可夫决策过程中，$ o_t = s_t^e = s_t^a $。 

> agent 的观测 $o_t$ 与环境的真实状态 $s_t^e$​ 完全相同。
>
> agent 的感知状态 $s_t^a$ 与环境的真实状态 $s_t^e$​​ 完全一致。



FOMDP(Fully Observable)：完全可观测马尔可夫决策过程的假设条件：

**agent 对环境的状态拥有完整且无误的感知，不存在信息不对称或部分可观测的情况。**



**部分可观测马尔可夫决策过程**

在实际应用中，这种理想情况往往并不成立，因此会引入部分可观测马尔可夫决策过程(partially observable)：

- $o_t$ 与 $s_t^e$ 不再相等，agent 只能接收到部分观测数据。
- agent 需要通过推理或内部状态表示来估计环境状态 $s_t^e$ ，从而形成其感知状态 $s_t^a$​ 。

> agent 的观测 $o_t$ 与环境的真实状态 $s_t^e$​ 不一定相等，agent 的观测数据只是部分的观测数据。
>
> agent 的感知状态 $s_t^a$ 与环境的真实状态 $s_t^e$​ 不一致。

在这种情况下，强化学习通常被建模成部分可观测马尔可夫决策过程的问题。

部分可观测马尔可夫决策过程是马尔可夫决策过程的一种泛化。部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是假设智能体无法感知环境的状态，只能知道部分观测值。

比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。部分可观测马尔可夫决策过程可以用一个七元组描述：$ (S, A, T, R, \Omega, O, \gamma) $。

其中 $ S $ 表示状态空间，为隐变量，$ A $ 为动作空间，$ T(s' | s, a) $ 为状态转移概率，$ R $ 为奖励函数，$ \Omega(o | s, a) $ 为观测概率，$ O $ 为观测空间，$ \gamma $ 为折扣系数。



不同的环境允许不同种类的动作。在给定的环境中，有效动作的集合经常被称为动作空间（action space）。



**观测与感知的区别**

**观测（Observation）：**智能体直接从环境中获取的信息，通常是部分或有限的。通常用 $o_t$ 表示。

- 观测是外在的、直接提供的。
- 在完全可观测环境中，观测等于环境的真实状态。
- 在部分可观测环境中，观测是环境真实状态的一个子集或投影。

**感知（Perception）：**智能体基于观测以及历史经验或内部推理形成的对环境状态的理解。通常用 $s_t^a$ 表示。

- 感知是内在的，包含了智能体的内部状态和计算过程。

- 在部分可观测的情况下，感知通常是一种对真实状态的估计。

- 感知可能包含记忆、滤波或预测等过程（如卡尔曼滤波、RNN 状态更新）。

观测是智能体从环境中接收到的外在信息，而感知是智能体对这些信息进行处理后形成的内部状态或估计。

两者的关系就像输入和内部处理的关系，观测为感知提供基础，感知则支持智能体的决策和行为。



#### 策略(Policy)：

随机性策略：就是 $\pi$ 函数，即 $\pi(a|s) = p(a_t = a | s_t = s)$。

输入一个状态 $s$，输出一个概率。这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。

确定性策略：是 agent 直接采取最有可能的动作，即 $a^* = \arg\max_a \pi(a | s)$。

> 差别就是：
>
> 假设输入一个 $s$，模型的输出是，向左，不动，向右，得到的分数分别是0.6，0.2，0.1
>
> 对于随机性策略，直接采样，有60%概率向左，20%的概率不动，10%的几率向右。
>
> 对于确定性策略，argmax(action)，100%的向左，直接采取向左。

通常 RL 中会采用 随机性策略，这是因为：

1. 促进探索：随机性策略允许 agent 以一定概率选择不同的动作，而不是只选择当前看似最优的动作，因为当前的最优解并不一定是全局的最优解。
2. 适应不确定性：在部分可观测或动态变化的环境中，环境的状态可能并不完全可知，随机性策略有助于 agent 在面对不确定性时做出更灵活的决策。
3. 优化需求：随机性策略 通常基于概率分布，这使得 策略梯度 方法在优化时具有平滑性，避免因为非连续性或零梯度问题而无法更新。



#### 价值函数：

**价值函数的值是对未来奖励的预测，用价值函数来评估状态的好坏。**
$$
V_\pi(s) \doteq \mathbb{E}_\pi \left[ G_t \mid s_t = s \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k r_{t+k+1} \mid s_t = s \right], \quad \text{对于所有的 } s \in S
$$
这段公式定义了马尔可夫决策过程中的 **状态值函数** ，它表示在给定策略 $\pi$ 下，从某个状态出发，智能体期望获得的累计奖励。



**公式的部分含义**

- **状态值函数** **:** $V_{\pi}(s)$是一个标量值，表示在策略 $\pi$下，从状态 $s$ 开始所能获得的未来总奖励的期望值。

- **累计奖励** **:** $G_t$是未来的累计奖励，定义为：

$$
G_t = \sum_{k=0}^\infty \gamma^k r_{t+k+1}
$$
表示从 $t$ 开始的 第 $k+1$ 步获得的即使奖励， $\gamma$ 为折扣因子用于权衡当前奖励和未来奖励的相对重要性。

- **期望值** **:**$$\mathbb{E}_\pi[\cdot]$$表示在策略 $\pi$ 下，计算未来奖励 $G_t$ 的期望值。

- **公式的整体解释：**表示在策略 $\pi$ 下，从状态 $s$ 开始，智能体所能期望获得的未来折扣累计奖励。



**公式的含义**

**状态值函数的作用：**用于评估状态 $s$ 的“好坏”，即从该状态出发遵循策略 $\pi$ 能获得的期望奖励。一个状态的值越高，说明从该状态出发遵循策略 $\pi$ 的回报越好。

**折扣因子的作用：**折扣因子控制了未来奖励的重要性。较小的 $\gamma$ 表示智能体更关注短期奖励，而较大的则表示未来奖励的重要性更高。

> 假设智能体玩一个游戏：
>
> 1.当前状态 $s$ 是“站在迷宫的起点”。
>
> 2.奖励 $r_t$ 表示智能体在每一步获得的分数。
>
> 3.未来奖励 $G_t$ 是所有接下来可能奖励的加权和。
>
> 4.如果策略 $\pi$ 是“总是往右走”，则 $V_{\pi}(s)$ 就表示“从起点按这个策略出发，期望总共能获得的分数”。



**另一种价值函数 ：$Q$ 函数**
$$
Q_\pi(s, a) \doteq \mathbb{E}_\pi \left[ G_t \mid s_t = s, a_t = a \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k r_{t+k+1} \mid s_t = s, a_t = a \right]
$$

- **$Q_\pi(s, a)$**: 表示在策略 $\pi$ 下，处于状态 $s$ 并采取动作 $a$ 后的期望累积回报。
- **$\mathbb{E}_\pi[\cdot]$**: 表示期望值，其中期望是基于策略 $\pi$ 计算的，即按照策略 $\pi$ 选择动作的概率分布。
- $G_t$: 表示从时间步 $t$ 开始的累积回报。
- $\mid s_t = s, a_t = a$: 表示我们已经知道当前状态是 $s_t = s$，并且在当前采取的动作是 $a_t = a$。



换句话说，$Q_\pi(s, a)$ 是一个条件期望，计算的是给定状态和动作后，未来累积回报的数学期望。

累积回报的公式展开：
$$
Q_\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k r_{t+k+1} \mid s_t = s, a_t = a \right]
$$
**累积回报的定义**: $G_t = \sum_{k=0}^\infty \gamma^k r_{t+k+1}$，累积和表示从 $t+1$ 时间步开始的所有折扣奖励。

因此，公式的右半部分实际上是在计算：**在给定 $s_t = s$ 和 $a_t = a$ 的情况下，未来所有折扣奖励的期望值**。



“如果在状态 $s$ 下选择动作 $a$，然后按照策略 $\pi$ 行动，在未来我可以期望获得多少累积回报？”

- **决策依据**: $Q_\pi(s, a)$ 是强化学习中用来指导决策的重要依据，智能体可以根据它来选择动作。例如，通过选择具有最大 $Q_\pi(s, a)$ 值的动作（即 $a = \arg\max_a Q_\pi(s, a)$）。
- **策略评估**: $Q_\pi(s, a)$ 也用于评估策略 $\pi$ 的好坏。好的策略应该使 $Q_\pi(s, a)$ 的值尽可能大。



#### 模型

![截屏2025-01-27 23.48.23](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-27 23.48.23.png)

模型决定了下一步的状态，下一步状态取决于当前的状态以及当前采取的动作。

它由状态转移概率和奖励函数两个部分组成，状态转移概率即：
$$
p_{ss'}^a = p \left(s_{t+1} = s' \mid s_t = s, a_t = a \right)
$$
**状态转移概率**表示从一个状态 $s_t = s$ 经过一个动作 $a_t = a$ 后，转移到下一个状态 $s_{t+1} = s{\prime}$ 的概率。

> 如果地面滑而机器人有可能滑倒，状态转移概率 $p_{ss{\prime}}^a$ 可能不是确定的，比如：
>
> p = 0.8：机器人按计划成功移动到目标位置 $s{\prime}$。
>
> p = 0.2：机器人因为滑倒移动到了其他意外的位置。



奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即
$$
R(s, a) = \mathbb{E} \left[ r_{t+1} \mid s_t = s, a_t = a \right]
$$
**奖励函数**定义了执行某个动作 a 后能够获得的奖励的期望值。

它描述了动作的好坏程度，指导 agent 选择更优的动作。



有了策略、价值函数和模型三个组成部分以后，就形成了一个马尔可夫决策过程。



#### value-based 和 policy-based

基于价值的智能体(value-based)：显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。

基于策略的智能体(policy-based)：直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。



把基于价值的智能体和基于策略的智能体结合起来就有了actor-critic。agent 把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。

> 比较基于策略和基于价值的



#### 探索和利用

在 RL 里面，探索和利用是两个很核心的问题。

探索即我们去探索环境，通过尝试不同的动作来得到最佳的策略（带来最大奖励的策略）。

利用即我们不去尝试新的动作，而是采取已知的可以带来很大奖励的动作。



在刚开始的时候，agent 不知道它采取了某个动作后会发生什么，所以它只能通过试错去探索，所以探索就是通过试错来理解采取的动作到底可不可以带来好的奖励。

利用是指我们直接采取已知的可以带来很好奖励的动作。所以这里就面临一个权衡问题，即怎么通过牺牲一些短期的奖励来理解动作，从而学习到更好的策略。



与监督学习任务不同，RL 的最终奖励在多步动作之后才能观察到，考虑比较简单的情形：最大化单步奖励，即仅考虑一步动作。

即使在这样的简单情形下，RL 仍与监督学习有显著不同，因为智能体需通过试错来发现各个动作产生的结果，而没有训练数据告诉智能体应当采取哪个动作。



想要最大化单步奖励需考虑两个方面：一是需知道每个动作带来的奖励，二是要执行奖励最大的动作。若每个动作对应的奖励是一个确定值，那么尝试遍所有的动作便能找出奖励最大的动作。

更一般的情形是，一个动作的奖励值是来自一个概率分布，仅通过一次尝试并不能确切地获得平均奖励值。



**K 臂赌博机**

单步强化学习对应一个理论模型，K 臂赌博机。

每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖励，即获得最多的硬币。



- 若仅为获知每个摇臂的期望奖励，则可采用**仅探索**：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖励期望的近似估计。

- 若仅为执行奖励最大的动作，则可采用**仅利用**：按下目前最优的（即到目前为止平均奖励最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。



仅探索法能很好地估计每个摇臂的奖励，却会失去很多选择最优摇臂的机会；

仅利用法则相反，没有很好地估计摇臂期望奖励，很可能经常选不到最优摇臂。

因此，这两种方法都难以使最终的累积奖励最大化。



事实上，探索（估计摇臂的优劣）和利用（选择当前最优摇臂) 这两者是矛盾的，因为尝试次数（总投币数）有限，加强了一方则自然会削弱另一方。

这就是强化学习所面临的**探索-利用窘境**。显然，想要累积奖励最大，则必须在探索与利用之间达成较好的折中。



## Chap3.马尔可夫决策过程

agent 得到环境的状态后，它会采取动作，并把这个采取的动作返还给环境。环境得到智能体的动作后，它会进入下一个状态，把下一个状态传给智能体。在 RL 中，agent 与环境就是这样进行交互的，这个交互过程可以通过马尔可夫决策过程来表示。

**马尔可夫决策过程是强化学习的基本框架.**



### 2.1 马尔可夫性质

![截屏2025-01-28 00.38.37](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 00.38.37.png)

一个状态的下一个状态，取决于当前状态，而与之前的状态无关。

对于 $h_t={s_1,s_2....}$ 包含了所有的状态，但是当前的转移从当前 $s_t$ 转到 $s_{t+1}$ 直接就等于之前所有的状态。

**当我们描述一个过程满足马尔可夫特征的时候，就是在说未来的转移跟过去是独立的，只取决于现在。**

马尔可夫性质是所有马尔可夫过程的基础。



#### 马尔可夫性质的具体定义

假设有一个随机过程 $X_t$，其中 $t$ 表示时间（可以是离散的或连续的），随机变量 $X_t $ 代表时间 $t$ 时的状态。

如果满足以下条件：

$$
P(X_{t+1} \mid X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} \mid X_t),
$$
即：给定当前状态 $X_t$ ，未来的状态 $X_{t+1}$ 与过去的状态 $X_{t-1}, \dots, X_0$ 无关，那么这个随机过程就是一个马尔可夫过程。

简单来说，马尔可夫性指的是当前状态完全决定未来状态，过去的信息可以被忽略。

> 具有马尔可夫性质并不是意味着这个随机过程就和历史完全没有关系了。
>
> 虽然 $t+1$ 时刻的状态只与 $t$ 状态有关，但是 $t$ 时刻的状态其实是包含了 $t-1$ 时刻状态的信息。
>
> 是通过这种链式的关系，历史的信息被传递到了现在。
>
> 马尔可夫性质简化了运算，因为只要当前的状态是可知的，所有历史信息就不重要了，利用当前状态信息就可以决定未来。



### 2.2 马尔可夫链

![截屏2025-01-28 00.49.23](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 00.49.23.png)

离散时间的马尔可夫过程也称为**马尔可夫链（Markov chain）**。

马尔可夫链是最简单的马尔可夫过程，其状态是有限的。

上图有 4 个状态，这 4 个状态在 *s*1*, s*2*, s*3*, s*4 之间互相转移。

从 *s*1 开始，*s*1 有 0.1 的概率继续存留在 *s*1 状态，有 0.2 的概率转移到 *s*2，有 0.7 的概率转移到 *s*4 。



可以使用状态转移矩阵来描述状态转移 $p(s_{t+1}=s'|s_t=s)$
$$
P = 
\begin{bmatrix}
P(s_1|s_1) & P(s_2|s_1) & \cdots & P(s_N|s_1) \\
P(s_1|s_2) & P(s_2|s_2) & \cdots & P(s_N|s_2) \\
\vdots     & \vdots     & \ddots & \vdots     \\
P(s_1|s_N) & P(s_2|s_N) & \cdots & P(s_N|s_N)
\end{bmatrix}
$$
这个状态转移矩阵 P 的作用类似于条件概率，表示我们知道当前在状态 $s_t$ 时，到达下面所有状态的概率。

$P$ 的第一行表示从 $s_1$ 转移到不同状态的概率



### 2.3 马尔可夫奖励过程

![截屏2025-01-28 01.02.18](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.02.18.png)

**马尔可夫奖励过程（Markov reward process, MRP）**是马尔可夫链加上奖励函数。

在马尔可夫奖励过程中，状态转移矩阵和状态都与马尔可夫链一样，只是多了奖励函数。

奖励函数 $R$ 是一个期望，表示当我们到达某一个状态的时候，可以获得多大的奖励。

另外定义了折扣因子 $γ$。如果状态数是有限的，那么 $R$ 可以是一个向量。



![截屏2025-01-28 01.05.07](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.05.07.png)

范围（horizon）是指一个回合的长度（每个回合最大的时间步数），它是由有限个步数决定的。

回报（return）可以定义为奖励的逐步叠加，假设时刻 $ t $ 后的奖励序列为 $ r_{t+1}, r_{t+2}, r_{t+3}, \cdots $，则回报为 
$$
\begin{equation} G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \gamma^3 r_{t+4} + \cdots + \gamma^{T-t-1} r_T \end{equation}
$$
 其中，$ T $ 是最终时刻，$ \gamma $ 是折扣因子，越往后得到的奖励，折扣越多。这表示更希望得到现有的奖励，对未来的奖励要打折扣。



#### 2.3.1 价值函数

当有了回报之后，就可以定义状态的价值了，就是价值函数（value function）。

**对于马尔可夫奖励过程，价值函数被定义成回报的期望，即 **
$$
V^t(s) = \mathbb{E}[G_t \mid s_t = s] = \mathbb{E}[r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots + \gamma^{T-t-1} r_T \mid s_t = s]
$$
其中，$ G_t $ 是之前定义的折扣回报。

对 $ G_t $ 取了一个期望，期望就是从这个状态开始，可能获得多大的价值。

所以期望也可以看成未来可能获得奖励的当前价值的表现，就是当我们进入某一个状态后，我们现在有多大的价值



**贝尔曼方程：**

![截屏2025-01-28 01.24.03](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.24.03.png)

从价值函数里面推导出**贝尔曼方程（Bellman equation）**: 
$$
\begin{equation} V(s) = \underbrace{R(s)}_{\text{即时奖励}} + \gamma \underbrace{\sum_{s' \in S} p(s' \mid s) V(s')}_{\text{未来奖励的折扣总和}} \end{equation}
$$
其中，$ s' $ 可以看成未来的某个状态，$ p(s' \mid s) $ 是指从当前状态转移到未来状态的概率。

这个式子的推导如下：
$$
\begin{align*}
V(s) &= \mathbb{E}[G_t | S_t = s] \\
     &= \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots | S_t = s] \\
     &= \mathbb{E}[R_t + \gamma (R_{t+1} + \gamma R_{t+2} + \ldots) | S_t = s] \\
     &= \mathbb{E}[R_t + \gamma G_{t+1} | S_t = s] \\
     &= \mathbb{E}[R_t + \gamma V(S_{t+1}) | S_t = s]
\end{align*}
$$
其中，即时奖励的期望就是当前奖励函数的输出：
$$
\mathbb{E}[R_t\mid S_t=s]=R(s)
$$
另一方面，剩余部分可以根据状态 $s$ 除法的转移概率得到，即：
$$
\mathbb{E}[\gamma V(S_{t+1}) | S_t = s]=\gamma{\sum_{s' \in S} p(s' \mid s) V(s')}
$$


贝尔曼方程中，$ V(s') $ 代表的是未来某一个状态的价值。

从当前状态开始，有一定的概率去到未来的所有状态，所以要把 $ p(s' \mid s) $ 写上去。我们、、得到了未来状态后，乘一个 $ \gamma $​，把未来的奖励打折扣。

$ \gamma \sum_{s' \in S} p(s' \mid s) V(s') $​ 可以看成未来奖励的折扣总和。 

**贝尔曼方程定义了当前状态与未来状态之间的关系。未来奖励的折扣总和加上即时奖励，就组成了贝尔曼方程。**



状态转移概率乘它未来的状态的价值，再加上它的即时奖励，就会得到它当前状态的价值。

贝尔曼方程定义的就是当前状态与未来状态的迭代关系。
$$
\begin{pmatrix} V(s_1) \\ V(s_2) \\ \vdots \\ V(s_N) \end{pmatrix} = \begin{pmatrix} R(s_1) \\ R(s_2) \\ \vdots \\ R(s_N) \end{pmatrix} + \gamma \begin{pmatrix} p(s_1 \mid s_1) & p(s_2 \mid s_1) & \cdots & p(s_N \mid s_1) \\ p(s_1 \mid s_2) & p(s_2 \mid s_2) & \cdots & p(s_N \mid s_2) \\ \vdots & \vdots & \ddots & \vdots \\ p(s_1 \mid s_N) & p(s_2 \mid s_N) & \cdots & p(s_N \mid s_N) \end{pmatrix} \begin{pmatrix} V(s_1) \\ V(s_2) \\ \vdots \\ V(s_N) \end{pmatrix}
$$


所有状态价值是向量 $[V(s_1), V(s_2), \cdots, V(s_N)]^\mathsf{T}$。

每一行来看，向量 $\mathbf{V}$ 乘状态转移矩阵里面的某一行，再加上它当前可以得到的奖励，就会得到它当前的价值。 

当我们把贝尔曼方程写成矩阵形式后，可以直接求解：
$$
\mathbf{V} = \mathbf{R} + \gamma \mathbf{P} \mathbf{V} \\
I \mathbf{V} = \mathbf{R} + \gamma \mathbf{P} \mathbf{V} \\
(I - \gamma \mathbf{P}) \mathbf{V} = \mathbf{R} \\
\mathbf{V} = (I - \gamma \mathbf{P})^{-1} \mathbf{R} \\
$$
 可以直接得到解析解： 
$$
\begin{equation} \mathbf{V} = (I - \gamma \mathbf{P})^{-1} \mathbf{R} \tag{2.15} \end{equation} 
$$


可以通过矩阵求逆把 $\mathbf{V}$ 的值直接求出来。但是问题是这个矩阵求逆的过程的复杂度是 $O(N^3)$。

当状态非常多的时候，比如从 10 个状态到 1000 个状态，或者到 100 万个状态，当我们有 100 万个状态的时候，状态转移矩阵就会是一个 100 万乘 100 万的矩阵，对这样一个大矩阵求逆是非常困难的。

所以这种通过解析解去求解的方法只适用于很小量的马尔可夫奖励过程。 



求解比较大规模的马尔可夫奖励过程中的价值函数，可以使用

1. 蒙特卡洛
2. 动态规划
3. 时序差分



#### 2.3.2 为什么需要折扣因子 $\gamma$

![截屏2025-01-28 01.10.05](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.10.05.png)



- 有些马尔可夫过程是带环的，它并不会终结，避免无穷的奖励。
- 我们并不能建立完美的模拟环境的模型，对未来的评估不一定是准确的，不一定完全信任模型，因为这种不确定性，所以我们对未来的评估增加一个折扣。我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励。
- 如果奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）。
- 最后，我们也更想得到即时奖励。有些时候可以把折扣因子设为 0 ($ \gamma = 0 $)，我们就只关注当前的奖励。我
  们也可以把折扣因子设为 1 ($ \gamma = 1 $)，对未来的奖励并没有打折扣，未来获得奖励与当前获得的奖励是一样的



### 2.4 马尔可夫决策过程

![截屏2025-01-28 01.37.17](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.37.17.png)

相对于马尔可夫奖励过程，马尔可夫决策过程多了决策（动作），其他的定义与马尔可夫奖励过程的是类似的。

此外，状态转移也多了一个条件，变成了 $ p(s_{t+1} = s' \mid s_t = s, a_t = a) $。

**未来的状态不仅依赖于当前的状态，也依赖于在当前状态智能体采取的动作。**

马尔可夫决策过程满足条件：
$$
\begin{equation} p(s_{t+1} \mid s_t, a_t) = p(s_{t+1} \mid h_t, a_t)\end{equation}
$$
 对于奖励函数，它也多了一个当前的动作，变成了 $ R(s_t = s, a_t = a) = \mathbb{E}[r_t \mid s_t = s, a_t = a] $。

当前的状态以及采取的动作会决定智能体在当前可能得到的奖励多少。



#### 2.4.1 策略 in MDP

策略定义了在某个状态 $s$ 可以采取什么样的动作，知道当前状态以后，可以把当前状态代入策略函数来得到一个概率，即
$$
\pi(a|s)=p(a_t=a|s_t=s)
$$


这表示在输入状态 $s$ 情况下，采取动作 $a$ 的概率。

概率代表在所有可能的动作里面怎样采取行动，比如可能有 0.7 的概率往左走，有 0.3 的概率往右走。

这种方式叫做**随机性策略**，对于随机性策略，每个状态输出的是关于动作的概率分布，然后根据这个分布进行采样得到一个动作。

还有一种**确定性策略**，直接确定输出某个动作。



**策略作用下的马尔可夫决策过程**

在马尔可夫决策过程中，也可以定义类似的价值函数。

此时的价值函数与策略有关，这表示对于两个不同的策略来说，在同一个状态下的价值很可能也是不同的。



已知马尔可夫决策过程和策略 $\pi$​，可以把马尔可夫决策过程转换成马尔可夫奖励过程。

在马尔可夫决策过程中，状态的转移依赖于**当前的状态 $s$ 和动作 $a$**，状态转移概率为：$p(s’ | s, a)$

即：在状态 $s$，采取动作 $a$ 后转移到状态 $s’$ 的概率。



#### 2.4.2 状态价值函数 $V_{\pi}(s)$

马尔可夫决策过程中的价值函数可定义为：

$$
V_{\pi}(s) = \mathbb{E}_{\pi} \left[ G_t \mid s_t = s \right]
$$
定义为从状态 $s$ 出发遵循策略 $\pi$ 能得到的期望回报。

当策略决定后，通过对策略进行采样来得到一个期望，计算出它的价值函数。



#### 2.4.3 动作价值函数 $Q^{\pi}(s,a)$

在 MDP 中，由于动作的存在，需要额外定义一个动作价值函数 $Q$。

**$Q$ 函数表示在状态 $s$ 下执行动作 $a$ 后，能获得的期望累积回报**，即
$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ G_t \mid s_t = s, a_t = a \right]
$$


**状态价值函数 $V$ 和动作价值函数 $Q$ 之间的关系：**

在使用策略 $\pi$ 中，**状态的价值 = 在该状态下基于策略采取所有动作的概率与相应的价值相乘再求和的结果：**
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) Q_{\pi}(s, a)
$$

> 在策略 $\pi$ 下，状态 $s$ 的值函数 $V_{\pi}(s)$ 是**所有可能动作值函数 $Q_{\pi}(s,a)$的加权平均**，权重为策略 $\pi$ 在状态 $s$ 下选择动作 $a$ 的概率 $\pi(a \mid s)$。



#### 2.4.3 状态价值函数 $V$ 和动作价值函数 $Q$ 的关系推导过程：

状态价值函数的定义为：
$$
V_{\pi}(s) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \mid s_t = s \right]
$$
这表示在状态  $s$  下，按照策略 $\pi$ 采取行动后，所能获得的预期总回报。

状态-动作价值函数定义为：
$$
Q_{\pi}(s, a) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \mid s_t = s, a_t = a \right]
$$
这表示在状态 $$ s $$ 下执行特定动作 $$ a $$，然后按照策略 $$ \pi $$ 继续行动，所能获得的预期总回报。



推导过程：
$$
V_{\pi}(s) = \mathbb{E}_\pi [ G_t \mid s_t = s ]
$$
由于策略 $$ \pi(a \mid s) $$ 给出了在状态 $$ s $$ 下选择动作 $$ a $$ 的概率，因此可以对所有可能的动作进行期望计算：
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) \mathbb{E}_\pi \left[ G_t \mid s_t = s, a_t = a \right]
$$
根据 $$ Q_{\pi}(s, a) $$ 的定义：
$$
Q_{\pi}(s, a) = \mathbb{E}_\pi \left[ G_t \mid s_t = s, a_t = a \right]
$$
代入得：
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) Q_{\pi}(s, a)
$$

**一个状态的价值等于该状态下所有可能动作的价值 $Q$ 的期望值，其中期望是按照当前策略 $$ \pi $$ 计算的**。



#### 2.4.4 对 $Q_{\pi}(s,a)$​对函数的贝尔曼方程进行推导：

$Q$ 函数的定义为：
$$
Q(s, a) = \mathbb{E}\left[ G_t \mid s_t = s, a_t = a \right]
$$
$$ Q(s, a) $$ 表示在状态 $$ s $$ 采取动作 $$ a $$ 后，未来能获得的期望累计回报。
$$
Q(s, a) = \mathbb{E}\left[ r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots \mid s_t = s, a_t = a \right]
$$

$$
Q(s, a) = \mathbb{E}\left[ r_{t+1} \mid s_t = s, a_t = a \right] + \gamma \mathbb{E}\left[ r_{t+2} + \gamma r_{t+3} + \gamma^2 r_{t+4} + \cdots \mid s_t = s, a_t = a \right]
$$

$$
Q(s, a) = R(s, a) + \gamma \mathbb{E}\left[ G_{t+1} \mid s_t = s, a_t = a \right]
$$

其中，$$ R(s, a) $$ 是奖励函数：$$R(s, a) = \mathbb{E} \left[ r_{t+1} \mid s_t = s, a_t = a \right]$$，即，在状态 $$ s $$ 采取动作 $$ a $$​ 后，所能获得的期望奖励。

而在马尔可夫决策过程中，$V$ 函数的定义是 $V(s_{t+1}) = \mathbb{E} \left[ G_{t+1} \mid s_{t+1} \right]$，所以：
$$
Q(s, a) = R(s, a) + \gamma \mathbb{E}\left[ V(s_{t+1}) \mid s_t = s, a_t = a \right]
$$
由于是对 $V(s')$ 求期望，所以需要对所有可能的 $s'$ 进行加权求和，权重是状态转移概率：
$$
Q(s, a) = R(s, a) + \gamma \sum_{s’ \in S} p(s’ \mid s, a) V(s’)
$$
**$Q$ 函数的贝尔曼方程：**
$$
Q(s,a)=R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V(s')
$$
其中前半部分 $R(s,a)$ 表示在状态 $s$ 下执行动作 $a$ 后，环境直接反馈的奖励。

而后半部分 $\gamma \sum_{s' \in S} p(s' \mid s, a) V(s')$ 表示未来奖励的期望值。



**$V$ 函数评估的是当前 $s$ 的整体好坏，比如离终点越近， $V$ 值越高**

**$Q$函数告诉在这个 $S$ 向某个方向走的具体价值，帮助选择最佳的路径**



#### 2.4.5 值函数 $V$ 的贝尔曼期望方程

对状态价值函数 $V^{\pi}$ 进行分解，可以得到一个类似于之前马尔可夫奖励过程的贝尔曼方程——贝尔曼期望方程
$$
V_{\pi}(s) = \mathbb{E}_{\pi} \left[ r_{t+1} + \gamma V_{\pi}(s_{t+1}) \mid s_t = s \right]
$$
对 $Q$ 函数也做类似的分解，得到 $Q$ 函数的贝尔曼期望方程
$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ r_{t+1} + \gamma Q_{\pi}(s_{t+1}, a_{t+1}) \mid s_t = s, a_t = a \right]
$$
 **贝尔曼期望方程定义了当前状态和未来状态的关联。**


$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) Q_{\pi}(s, a)
$$

$$
Q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V_{\pi}(s')
$$

(38)和(39)代表状态价值函数与 Q 函数之间的关联。



把(39)代入(38)可以得到：
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) \left( R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V_{\pi}(s') \right)
$$
表示当前状态的价值与未来状态价值之间的关联，描述了在策略 $$ \pi $$ 下，状态 $$ s $$ 的值函数 $$ V_{\pi}(s) $$ 是如何递归定义的

> 该方程的意义是：**状态 $$ s $$ 的期望回报等于在该状态下所有可能的动作的加权平均回报**，其中：
>
> **策略 $$ \pi(a \mid s) $$**：表示在状态 $$ s $$ 下采取动作 $$ a $$ 的概率。
>
> **立即奖励 $$ R(s, a) $$**：在状态 $$ s $$ 采取动作 $$ a $$ 所获得的期望奖励：
>
> **未来回报 $$ \sum_{s’ \in S} p(s’ \mid s, a) V_{\pi}(s’) $$**：
>
> $$ p(s’ \mid s, a) $$ 表示从 $$ s $$ 采取 $$ a $$ 之后转移到 $$ s’ $$ 的概率。
>
> $$ V_{\pi}(s’) $$ 表示从 $$ s’ $$ 开始的期望回报。
>
> 表示所有可能的下一状态 $$ s’ $$ 的值函数 $$ V_{\pi}(s’) $$ 的加权和。



**举例说明**

假设在某个状态 $$ s $$：可以选择 $$ a_1 $$ 或 $$ a_2 $$ 两个动作。策略 $$ \pi $$ 规定：以 70% 概率选择 $$ a_1 $$，30% 概率选择 $$ a_2 $$。

立即奖励：$$ R(s, a_1) = 5 $$，$$ R(s, a_2) = 2 $$。

状态转移概率：

- $ a_1 $ 导致 80% 的概率转移到 $$ s_1 $$（$ V(s_1) = 10 $），20% 的概率转移到 $$ s_2 $$（$$ V(s_2) = 5 $$）。

- $$ a_2 $$ 导致 50% 的概率转移到 $$ s_1 $$，50% 的概率转移到 $$ s_2 $$。

假设折扣因子 $$ \gamma = 0.9 $$​。
$$
V_{\pi}(s) = 0.7 \left( 5 + 0.9(0.8 \times 10 + 0.2 \times 5) \right) + 0.3 \left( 2 + 0.9(0.5 \times 10 + 0.5 \times 5) \right)=11.48
$$
这个值表示的意义是：**如果从 $$ s $$ 开始，按照策略 $$ \pi $$ 采取动作，长期回报的期望值是 11.48**。



值函数 $V$ 的贝尔曼期望方程是，**当前状态的值是未来所有可能路径的加权总和**。通过递归关系，可以计算出所有状态的值，从而评估不同策略的好坏。

在 RL 的策略评估中，通常可以利用这个方程迭代计算 状态值函数 $$ V_{\pi}(s) $$，然后基于它进行策略改进，从而求解最优策略。



#### 2.4.6 $Q$ 函数的贝尔曼期望方程

把(38)代入(39)可以得到
$$
Q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) \sum_{a' \in A} \pi(a' \mid s') Q_{\pi}(s', a')
$$
这表示当前时刻的 $Q$ 函数与未来时刻的 $Q$ 函数之间的关联

这个式子体现了 RL 中的递归思想：当前的 $Q$ 值 = 立即奖励 + 未来 $Q$ 值的折扣折扣加权和。

未来的回报取决于所有可能的下一个状态 $s{\prime}$ 以及在 $s{\prime}$ 处可能采取的所有动作 $a{\prime}$。



#### 2.4.7 将 MDP 转化为 MRP

已知策略 $\pi$​​ 后，可以通过**将动作的影响加权平均**，将马尔可夫决策过程转换为马尔可夫奖励过程。

> 确定策略以后，所有动作的概率进行加权，得到的奖励和可以认为是一个 MRP 在该状态 $s$ 下的奖励。

这涉及两个部分：**状态转移函数**和**奖励函数**。



**(1) 状态转移函数**

原本的状态转移函数依赖于动作 $a$：$$p(s’ | s, a)$$。

已知策略 $\pi(a|s)$ 后，可以对动作 $a$ 加权求和，得到**不依赖于动作的状态转移函数**：
$$
P_{\pi}(s’|s) = \sum_{a \in A} \pi(a|s)~ p(s’|s, a)
$$
这个式子表达的意思是：基于策略 $\pi$，计算从状态 $s$ 转移到状态 $s’$ 的概率，考虑了所有可能的动作及其概率。



**(2) 奖励函数**

原本的奖励函数依赖于动作 $a$：$R(s, a)$。

同样通过策略 $\pi(a|s)$ 对动作 $a$ 加权求和，得到**不依赖于动作的奖励函数**：
$$
r_{\pi}(s) = \sum_{a \in A} \pi(a|s) R(s, a)
$$
这个式子表达的意思是在状态 $s$ 中，**按照策略 $\pi$ 选择动作时，所有可能动作的即时奖励的期望值**。
即，根据策略 $\pi$ 的动作概率分布，对每个动作的即时奖励进行加权求和。



#### 2.4.8 MDP 和 MRP 的区别

![截屏2025-01-28 11.45.54](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 11.45.54.png)

**MRP**

MRP 是一种简化的随机过程，不含动作，只有状态转移和奖励。MRP 是 MDP 在给定策略 $\pi$ 后的简化形式。

马尔可夫奖励过程的状态转移是直接决定的。

比如当前状态是 *s*，那么直接通过转移概率决定下一个状态是什么。



**MDP**

MDP 是一个包含动作选择的随机过程，用于描述 agent 在环境中通过采取动作、与环境交互以获得奖励的过程

但对于马尔可夫决策过程，它的中间多了一层动作 $a$ ，即 agent 在当前状态的时候，首先要决定采取某一种动作，到达某一个黑色的节点。到黑色的节点后，因为有一定的不确定性，所以当 agent 当前状态以及智能体当前采取的动作决定过后，agent 进入未来的状态其实也是一个概率分布。

在当前状态与未来状态转移过程中多了一层决策性，这是马尔可夫决策过程与之前的马尔可夫奖励过程很不同的一点。在马尔可夫决策过程中，动作是由 agent 决定的，agent 会采取动作来决定未来的状态转移



**备份图(回溯图)**

在 RL 中，**备份图（Backup Diagram）** 是一种可视化工具，描述价值函数（如 $V(s)$ 或 $Q(s,a)$）的更新过程。

通过图形化的方式展示**当前状态（或状态-动作对）与后续状态（或动作）之间的依赖关系**，帮助理解算法如何将未来信息“回溯”到当前状态的更新中。

![截屏2025-01-28 13.57.19](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 13.57.19.png)

上图有两层加和。第一层加和是对叶子节点进行加和，往上备份一层，我们就可以把未来的价值（$s'$ 的价值）备份到黑色的节点。

第二层加和是对动作进行加和，得到黑色节点的价值后，往上备份一层，得到根节点的价值，即当前状态的价值。



对于这个图的上半部分，从 $a$ 到 $v_{\pi}(s)$，给出了状态价值函数 $Q$ 和 $V$ 函数的关系
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) Q_{\pi}(s, a)
$$
下半部分，计算 $Q$ 函数为：
$$
Q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V_{\pi}(s')
$$
把下面这个式子代入上面这个 $V$ 可以得到：
$$
V_{\pi}(s) = \sum_{a \in A} \pi(a \mid s) \left( R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V_{\pi}(s') \right)
$$
所以备份图定义了未来下一时刻的状态价值函数与上一时刻的状态价值函数之间的关联。



![截屏2025-01-28 14.03.46](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 14.03.46.png)

对于 $Q$​ 函数同理，根节点是 Q 函数的一个节点。Q 函数对应于黑色的节点。下一时刻的 Q 函数对应于叶子节点，有 4 个黑色的叶子节点。

这里也有两层加和。第一层加和先把叶子节点从黑色节点推到空心圆圈节点，进入到空心圆圈结点的状态。

当到达某一个状态后，再对空心圆圈节点进行加和，这样就把空心圆圈节点重新推回到当前时刻的 Q 函数。
$$
V_{\pi}(s') = \sum_{a' \in A} \pi(a' \mid s') Q_{\pi}(s', a')
$$

$$
Q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) \sum_{a' \in A} \pi(a' \mid s') Q_{\pi}(s', a')
$$

**策略评估**

策略评估是给定 马尔可夫决策过程和策略，评估可以获得多少价值，即对于当前策略，可以得到多大的价值。

可以直接把**贝尔曼期望备份（Bellman expectation backup）** ，变成迭代的过程，反复迭代直到收敛。

这个迭代过程可以看作**同步备份（synchronous backup）** 的过程。
$$
V^{t+1}(s) = \sum_{a \in A} \pi(a \mid s) \left( R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V^t(s') \right)
$$
当得到上一时刻的 $V_t$ 的时候，就可以通过递推的关系推出下一时刻的值。

反复迭代，最后 $V$ 的值就是从 $V_1,V_2$ 到最后收敛之后的值 $V_{\pi}$。

$V_{\pi}$就是当前给定的策略 $\pi$ 对应的价值函数。



**马尔可夫决策过程控制**

策略评估是指给定马尔可夫决策过程和策略，我们可以估算出价值函数的值。

若只有马尔可夫决策过程，该如何寻找最佳的策略，从而得到最佳价值函数？ 

最佳价值函数的定义为
$$
\begin{equation} V^*(s) = \max_{\pi} V_{\pi}(s) \end{equation}
$$
最佳价值函数是指，搜索一种策略 $\pi$ 让每个状态的价值最大。

$V^*$ 就是到达每一个状态，它的值的最大化情况。在这种最大化情况中，得到的策略就是最佳策略，即 
$$
\begin{equation} \pi^*(s) = \arg\max_{\pi} V_{\pi}(s) \end{equation}
$$
最佳策略使得每个状态的价值函数都取得最大值。

所以如果可以得到一个最佳价值函数，就可以认为某个马尔可夫决策过程的环境可解。

在这种情况下，最佳价值函数是一致的，环境中可达到的上限的值是一致的，但这可能存在多个最佳策略，多个最佳策略可以取得相同的最佳价值。 ![截屏2025-01-28 17.01.31](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 17.01.31.png)

当取得最佳价值函数后，我们可以通过对 $Q$ 函数进行最大化来得到最佳策略： 
$$
\begin{equation} \pi^*(a \mid s) =  \begin{cases}  1, & a = \arg\max_{a \in A} Q^*(s, a) \\ 0, & \text{其他} \end{cases} \end{equation}
$$
当 $Q$ 函数收敛后，因为 $Q$​ 函数是关于状态与动作的函数，所以如果在某个状态采取 某个动作，可以使得 $Q$ 函数最大化，那么这个动作就是最佳的动作。

如果能优化出一个 Q 函数 $Q^*(s,a)$​，就可以直接在 $Q$ 函数中取一个让 $Q$ 函数值最大化的动作的值，就可以提取出最佳策略。



搜索最佳策略可以采用穷举，但是效率太低，一般可以采用**策略迭代**和**价值迭代**。



### 2.5 蒙特卡洛



### 2.6 策略迭代(Policy Iteration)



![截屏2025-01-29 14.08.48](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-29 14.08.48.png)

可以把 $Q$ 函数看成一个 $Q$ 表格：横轴是它的所有状态，纵轴是它的可能的动作。

当得到 $Q$ 函数，$Q$ 表格也就得到了。对于某个状态，每一列取最大的值，最大值对应的动作就是它现在应该采取的动作。

**$\arg\max$ 操作是指在每个状态里面采取一个动作，这个动作是能使这一列的 $Q$ 函数值最大化的动作**



**贝尔曼最优方程**

通过对上面的 $Q$ 函数取贪心的操作，会得到更好或不变的策略。当停止改进的时候，会的得到一个最佳策略，取让 $Q$ 函数值最大化的动作， $Q$ 函数就会直接变成价值函数，即：
$$
\begin{equation} Q_{\pi}(s, \pi'(s)) = \max_{a \in A} Q_{\pi}(s, a) = Q_{\pi}(s, \pi(s)) = V_{\pi}(s) \end{equation} 
$$
我们也就可以得到贝尔曼最优方程
$$
 \begin{equation} V_{\pi}(s) = \max_{a \in A} Q_{\pi}(s, a) \end{equation} 
$$
贝尔曼最优方程表明：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的回报的期望。

当马尔可夫决策过程满足贝尔曼最优方程的时候，整个马尔可夫决策过程已经达到最佳的状态。

只有当整个状态已经收敛后，我们得到最佳价值函数后，贝尔曼最优方程才会满足。

满足贝尔曼最优方程后，我们可以采用最大化操作，即
$$
\begin{equation} V^*(s) = \max_{a} Q^*(s, a) \end{equation} 
$$
取让 $Q$ 函数值最大化的动作对应的值就是当前状态的最佳的价值函数的值。 



**$Q$ 函数的贝尔曼方程，$Q$ 函数之间的转移**
$$
\begin{align*} Q^*(s, a) &= R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V^*(s') \\ &= R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) \max_{a'} Q^*(s', a') \end{align*} 
$$
RL 中的 $Q$ 学习是基于贝尔曼最优方程来进行的，当取 $Q$ 函数值最大的状态（$\max_{a'} Q^*(s', a')$）的时候可得
$$
 \begin{equation} Q^*(s, a) = R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) \max_{a'} Q^*(s', a') \end{equation} 
$$
**状态价值函数的转移：**
$$
\begin{align*} V^*(s)  &= \max_{a} \left( R(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V^*(s') \right) \end{align*}
$$

> 取 max 操作选择当前最优动作，确保每一步决策都可以最大化长期的奖励。







### 2.10 计算马尔可夫奖励过程价值

![截屏2025-01-28 01.34.13](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.34.13.png)



**1.蒙特卡洛**

![截屏2025-01-28 01.34.46](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.34.46.png)



**2.动态规划**

![截屏2025-01-28 01.36.38](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-28 01.36.38.png)







## Chap4.动态规划算法

基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。

动态规划会保存已解决的子问题的答案，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。



基于动态规划的强化学习算法主要有两种：一是**策略迭代**（policy iteration），二是**价值迭代**（value iteration）

其中，策略迭代由两部分组成：**策略评估**（policy evaluation）和**策略提升**（policy improvement）。



策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程；

而价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。



### 4.1 策略迭代

策略迭代由两个步骤组成：策略评估和策略改进。

**第一个步骤是策略评估**，当前我们在优化策略 $\pi$，在优化过程中得到一个最新的策略。我们先保证这个策略不变，然后估计它的价值，即给定当前的策略函数来估计状态价值函数。

**第二个步骤是策略改进**，得到状态价值函数后，我们可以进一步推算出它的 $Q$ 函数。得到 $Q$ 函数后，直接对 $Q$ 函数进行最大化，通过在 $Q$ 函数做一个贪心的搜索来进一步改进策略。这两个步骤一直在迭代进行。



#### 4.1.1 策略评估



**贝尔曼期望方程**
$$
V^{\pi}(s) = \sum_{a \in A} \pi(a \mid s) \left( r(s, a) + \gamma \sum_{s' \in S} p(s' \mid s, a) V^{\pi}(s') \right)
$$


**知道奖励函数 $r(s,a)$ 和状态转移函数时，就可以根据下一个状态的价值来计算当前状态的价值。**

这符合动态规划的思想，把下一个可能的状态的价值当做一个子问题，把计算当前状态的价值看作是当前的问题。

知道子问题的解以后，可以利用子问题的解求解当前的问题。



可以从任意一个初始的 $V_0(s)$ 开始，不断的应用贝尔曼期望方程进行更新，直到收敛

在实际操作中，可以设定一个阈值 $\theta$，当所有状态价值变化量小于 $\theta$ 时，则认为算法收敛了。

一般可以设置 $\theta=1e-6$ ，确保价值基本稳定。



**进行策略评估的目的是计算出 $\pi$ 下每个状态的价值函数，用于评估策略的好坏，决定是否要改进策略。**



#### 4.1.2 策略提升



通过上一步的策略评估，已经知道价值 $V^{\pi}$ 。

假设 agent 在状态 $s$ 下采取动作 $a$，之后的动作仍然遵循 $\pi$，得到的期望回报就是动作价值 $Q^{\pi}(s,a)$。

若 $Q^{\pi}(s,a)$ 大于 $V^{\pi}(s)$，这意味着策略 $\pi$ 可能不是最优的，至少在状态 $s$ 下可以改进策略，让 agent 直接选择 $a$ 来提高整体回报。



这个假设只是针对一个状态，假设存在一个确定性策略 $\pi'$ 在任意的 $s$ 下，都满足 $Q^{\pi}(s,\pi'(s))>V^{\pi}(s)$

所以在任意状态 $s$ 下，都有 $V^{\pi'}(s)\geq V^{\pi}(s)$，这就是**策略提升定理**。



策略提升定理是 RL 中策略迭代的理论基础，说明了**如果一个策略在所有状态下选择的动作都比当前策略的动作价值更优，则新策略的回报一定不低于旧策略**。

策略提升定义说明了，新策略 $\pi'$ 至少会和旧的策略 $\pi$ 一样好，至少不会变差，采用新策略 $\pi'$ 的期望回报一定不会小于旧策略 $\pi$ 下的期望回报。



有了策略提升定理，就可以贪心地在每一个状态 都选择当前的动作价值最大的动作，用公式表示就是：
$$
\pi'(s) = \arg\max_{a} Q^{\pi}(s, a) = \arg\max_{a} \{ r(s,a) + \gamma \sum_{s'} P(s'|s,a) V^{\pi}(s') \}
$$
根据贪心策略 选择动作从而得到新策略的过程称为策略提升。

当策略提升之后得到的策略 $\pi'$ 和之前的策略 $\pi$ 一样时，说明策略迭代达到了收敛，此时 $\pi$ 和 $\pi'$ 就是最优策略。



#### 4.1.3 策略提升定理的证明

策略提升定理的前提是：$V^{\pi}(s)\leq Q^{\pi}(s,\pi'(s))$

当
$$
V^{\pi}(s) \leq Q^{\pi}(s, \pi'(s))=
\mathbb{E}_{\pi'} [R_t + \gamma V^{\pi}(S_{t+1}) | S_t = s]
$$
所以
$$
V^{\pi}(s) \leq Q^{\pi}(s, \pi'(s))
\leq \mathbb{E}_{\pi'} [R_t + \gamma Q^{\pi}(S_{t+1}, \pi'(S_{t+1})) | S_t = s]
$$
则：
$$
V^{\pi}(s) \leq Q^{\pi}(s, \pi'(s))
\leq \mathbb{E}_{\pi'} [R_t + \gamma Q^{\pi}(S_{t+1}, \pi'(S_{t+1})) | S_t = s]

\\
= \mathbb{E}_{\pi'} [R_t + \gamma R_{t+1} + \gamma^2 V^{\pi}(S_{t+2}) | S_t = s]


\\
\leq \mathbb{E}_{\pi'} [R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 V^{\pi}(S_{t+3}) | S_t = s]

\\
\vdots
\\
\leq \mathbb{E}_{\pi'} [R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots | S_t = s]
\\
= V^{\pi'}(s)
$$
每一个时间步都利用到局部动作价值优先 $V^{\pi}(S_{t+1}) \leq Q^{\pi}(S_{t+1}, \pi'(S_{t+1}))$。



#### 4.1.4 策略迭代算法

评估-->提升-->评估-->提升-->评估-->提升-->评估-->提升

![截屏2025-01-30 17.51.52](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-30 17.51.52.png)



### 4.2 价值迭代

#### 4.2.1 价值迭代

在价值迭代中，不存在显示的策略，只是维护一个状态价值函数
$$
V^*(s) = \max_{a \in \mathcal{A}} \left\{ r(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s, a) V^*(s') \right\}
$$
其中：

$V^*(s)$：表示状态 $s$ 的最优价值

$P(s'\mid s,a)$：在状态 $s$ 采取动作 $a$ 转移到状态 $s'$ 的概率

这个公式的意思是：

**状态** $s$ **的最优价值等于在所有可能动作  $a$ 中，选择能够最大化未来收益的动作对应的期望回报。**

![截屏2025-01-30 23.11.32](/Users/n/Library/Application Support/typora-user-images/截屏2025-01-30 23.11.32.png)



价值迭代的核心思想是：

1.**初始化**：所有状态的价值设为 0（或者任意值）。

2.**更新状态价值**：使用 **贝尔曼方程** 迭代更新：
$$
V(s) \leftarrow \max_a \sum_{s{\prime}} P(s{\prime} | s, a) \left[ R(s, a) + \gamma V(s{\prime}) \right]
$$
3.**检查收敛**：如果所有状态的价值变化（误差）小于某个阈值 ，则停止迭代。

4.**提取最优策略**：根据最终的价值函数确定最优策略：
$$
\pi^*(s) = \arg\max_a \sum_{s{\prime}} P(s{\prime} | s, a) \left[ R(s, a) + \gamma V(s{\prime}) \right]
$$
每个状态 $s$ 选择能够最大化回报的动作 $a$



#### 4.2.2 价值迭代和策略迭代的区别



**策略迭代**

核心思想：交替进行策略评估 和 策略提升，知道策略收敛

策略评估：计算当前策略下的状态价值函数 $V(s)$ ，直到收敛

策略提升：根据 $V(s)$ 选择新的最优策略 $\pi(s)$

收敛方式：反复执行评估和提升，知道策略不变

> 对于策略迭代，先用 policy_eval 计算当前策略的价值函数 $V(s)$
>
> 再用policy_improvement 更新策略 $\pi(s)$​，直到策略不再变化

```python
def policy_iteration(self):
    while True:
        self.policy_evaluation()  # 评估当前策略
        old_pi = copy.deepcopy(self.pi)  # 备份旧策略
        new_pi = self.policy_improvement()  # 提升策略
        if old_pi == new_pi:  # 策略不变，收敛
            break
```

**策略迭代类似于，先玩一段时间(策略评估)，再总结出更好的策略(策略提升)**



**价值迭代**

核心思想：直接迭代贝尔曼最优方程，不断逼近最优价值

策略评估：不完全进行策略评估，而是 只进行一次贝尔曼更新

策略提升：通过贝尔曼最优方程隐式地进行策略提升

收敛方式：只更新价值函数，直到价值收敛，最后一次性提取最优策略

> 直接更新状态价值 $V(s)$，使用 max 选择最优动作
>
> 价值收敛后，再用 get_policy 提取最终策略

```python
def value_iteration(self):
    while True:
        max_diff = 0
        new_v = [0] * self.env.ncol * self.env.nrow
        for state in range(self.env.ncol * self.env.nrow):
            qsa_list = []
            for action in range(4):  # 遍历所有动作
                qsa_list.append(self.compute_qsa(state, action))
            new_v[state] = max(qsa_list)  # 直接选择最大 Q(s, a)
            max_diff = max(max_diff, abs(new_v[state] - self.v[state]))
        self.v = new_v
        if max_diff < self.theta:  # 价值收敛
            break
    self.get_policy()  # 最后提取策略
```

**价值迭代类似于，每次问自己下一个最佳的决策是什么，从而收敛到最优解**



若状态空间比较小，策略迭代更合适

若状态空间很大，价值迭代更合适，避免了不必要的策略评估



### 4.3 动态规划的优缺点

**✅ 动态规划的优点**

**1️⃣ 能找到最优解**

DP 依赖于 **贝尔曼方程 **，理论上可以找到全局最优的策略（即最优价值函数）。

适用于**已知环境模型**（Model-based），可以精确计算出每个状态的最优动作。



**2️⃣ 收敛性强**

DP 方法是**迭代更新**，每次更新都基于上次的值，**一定会收敛到最优解**。

比起蒙特卡洛方法或 Q-learning 等方法，DP 的数学性质更稳定，收敛速度更可控。



**3️⃣ 可扩展到策略控制**

通过**策略迭代**或**价值迭代**，可以直接求解最优策略，而不需要反复试错（不像蒙特卡洛方法或深度强化学习）。

适用于**离散状态和动作空间**，比如网格世界（Grid World）或棋盘游戏。



**4️⃣ 利用环境模型**

DP 需要 **已知状态转移概率 P(s’ | s, a)**，这意味着可以精确计算值函数，而不需要像无模型方法（Model-free）那样进行大量交互学习。如果 MDP 的**状态转移是确定的**，DP 方法能非常高效地找到最优策略。



**❌ 动态规划的缺点**

**1️⃣ 计算复杂度高**

**时间复杂度高**：DP 需要更新所有状态的价值（$O(S^2 A)$ 级别计算），在大规模问题上不现实。

**空间复杂度高**：需要存储所有状态的**价值函数**和**策略表**，对于高维度或连续状态的 MDP 会变得不可行。



**2️⃣ 需要完整的环境模型**

DP 需要**已知状态转移概率 P(s’ | s, a)**，但在实际应用中，这个信息往往是**未知的**。

许多 RL 任务，如机器人控制，没有明确的状态转移模型，因此**无法直接应用 DP**。



**3️⃣ 不适用于连续状态空间**

DP **适用于离散状态**，如果状态是连续的（如物理系统、金融市场），则必须进行**离散化**，这会引入误差并增加计算量。



**4️⃣ 无法主动探索**

DP 只是在已知的状态转移模型下计算价值，**不会主动探索环境**。

在**强化学习任务中，探索（Exploration）是核心问题**，但 DP 只能在已知环境中计算最优策略，不能适用于无模型环境（Model-free）。



### 4.4 总结

若状态空间比较小，如网格世界，在代码中的例子即是网格世界，适合用DP。

需要最优策略，也适合用 DP。

此外，已知状态转移模型，也适合用 DP。



总的来说，DP 适用于小规模、确定的、已知的环境的马尔可夫决策问题，能求得最优策略。

但 DP 的计算量大，不适合大规模或model-free 的 RL 任务。

实际在 RL 中，DP 不是主流方法，而是用于理论分析或作为 RL 算法的对比基准。



## Chap5.时序差分算法

不直到环境的奖励函数和状态转移函数，需要直接使用和环境交互的过程中采样到的数据进行学习的情况下，不能用 DP。

也就是说，对于某些真实的环境，马尔可夫决策过程的状态转移概率写不出来，就不能DP。在这种情况下，agent 只能和环境交互学习，也就是 model-free rl。无模型的强化学习。



### 5.1 时序差分方法



**时序差分算法用当前获得的奖励加上下一个状态的价值估计来作为当前状态会获得的回报：**

核心思想：通过一步预测来近似累积回报



#### 5.1.1 TD 更新公式的推导

$$
V(s) = \mathbb{E}\pi [ G_t \mid s_t = s ]

$$

其中 $G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\cdots$，直接计算$G_t$很复杂，使用一步预测的近似： 
$$
G_t \approx r_{t+1} + \gamma V(s_{t+1})
$$
其中 $V(s_{t+1})$ 是对下一个状态 $s_{t+1}$ 的价值估计，表示从下一个状态开始，agent 预期能获得的未来奖励的期望



定义 TD 误差：**衡量估计值和目标之间的差距**
$$
\delta_t = \underbrace{r_{t+1}+\gamma V(s_{t+1})}_{\text{一步预测的近似}}\ \ \ \ \ - \underbrace{V(s_t)}_{\text{目标/实际的价值函数}}
$$
基于 TD 误差进行更新：
$$
V(s_t) \leftarrow V(s_t) + \alpha \delta_t
$$
其中 $\alpha$ 为学习率，和梯度下降法在目标优化和误差修正方面很类似，**TD(0) 的更新公式：**
$$
V(s_t) \leftarrow V(s_t) + \alpha \left( r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \right)
$$


TD 学习的核心：基于当前的经验和已有的估计进行自我更新。

不需要完整模型，$\delta$ 衡量的是当前观察鱼当前估计之间的差距。



#### 5.1.2 Sarsa 更新公式的推导

可以用 TD 算法来进行策略提升，在不知道奖励函数和状态转移函数的情况下。

**Sarsa(State-Action-Reward-State-Action)**


$$
Q(s_t, a_t) = \mathbb{E}[G_t \mid s_t, a_t]
$$
其中  $G_t$  是总回报，$G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots$

$$
Q(s_t, a_t) = \mathbb{E}\left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) \right]
$$
同理：定义误差 $\delta_t$
$$
\delta_t = r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)
$$
更新公式：
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \delta_t
$$




### 5.2 Sarsa 算法

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
$$

用贪心思想，选取某个状态下动作价值最大的动作，$\arg \max Q(s,a)$



问题1：用贪心的话，可能会导致某些状态动作对 $(s,a)$ 永不发生，以至于无法估计这个 $(s,a)$ 会不会更好

**解决办法： $\epsilon$-贪婪**
$$
\pi(a|s) = 
\begin{cases} 
\epsilon / |A| + 1 - \epsilon & \text{如果 } a = \arg\max_{a'} Q(s, a') \\
\epsilon / |A| & \text{其他动作}
\end{cases}
$$
有 $\epsilon$ 的概率从动作空间中随机采取一个动作



问题2：传统策略迭代中，必须等到策略评估收敛后，才能进行更新策略，选择最优动作。在状态空间很大的时间，效率很低。TD 虽然减少了对完成回报的依赖，但是仍然需要足够的样本收敛到精确的价值函数。

**解决办法：广义策略迭代**

不需要等到策略评估完全收敛，就可以开始进行策略改进。每次采样后立即进行价值更新和策略更新。

实际上，价值迭代 是 广义策略迭代的一个特例。

在价值迭代中，每次只进行一次策略评估，即更新一次 $V(s)$，然后立即进行策略改进。
$$
V(s) \leftarrow \max_a \sum_{s{\prime}, r} P(s{\prime}, r \mid s, a) \left[ r + \gamma V(s{\prime}) \right]
$$


> 为什么可以这样，为什么这样会有效？
>
> 1. 策略评估不需要完美：就算价值函数不够完美，新的策略还是会比现在的好，至少不会变差
> 2.  TD 学习或 Q-learning 甚至可以基于单个样本进行局部更新和策略改进，依然能收敛到最优解。
> 3. 相互促进：广义策略迭代中，评估和改进不断循环、互相推动，逐渐逼近最优策略。



**Sarsa 具体算法如下：**

![截屏2025-02-04 00.39.29](/Users/n/Library/Application Support/typora-user-images/截屏2025-02-04 00.39.29.png)



### 5.3 Q-table

用 Q-table 记录每个 (s,a) 的预期回报。

Q-table 的行数等于状态空间的大小，列数等于动作空间的大小。

在 Sarsa 中，agent 根据当前的 Q 表，选择下一个动作，



**Q-table 的缺点：**

1. 空间和时间复杂度：当状态空间或动作空间很大的时候，Q-table 的维度很大
2. 难以处理连续状态和动作：Q-table 主要还是用在离散的状态和动作空间，例如代码中的悬崖环境，状态 $s$ 有限，动作 $a$ 也只有上下左右四个。



### 5.4 nstep-Sarsa 算法



TD 用 **一步预测** 来估算回报，利用 及时奖励 和下一个状态的价值估计来更新当前状态的价值
$$
V(s_t) \leftarrow V(s_t) + \alpha \left( r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \right)
$$

1. TD 只关注 当前 和下一个状态的价值估计，只需要一步的信息，方差比较小(相对于蒙特卡洛)

2. 但是 TD 是有偏的，TD 并不像蒙特卡罗通过完整的回报来计算，并不是直接观测到的实际值



MC 依赖于 完整的状态信息，在回合结束的时候才计算累积的回报

1. MC 使用的是实际的回报，MC 是无偏的
2. 但是 MC 的方差大，毕竟一整个回合中每一步的奖励都是不确定的，最后还要加起来，所以最终的估计受随机因为的影响比较大，简单来说就是方差大，可能导致学习过程不稳定。



MC 的优势 + TD 的优势 = **nstep-Sarsa**

多步时序差分就是使用 $n$ 步的奖励，然后使用之后状态的价值估计
$$
G_t = r_t + \gamma r_{t+1}+\cdots+\gamma^nQ(s_{t+n},a_{t+n})
$$
对于多步 Sarsa 的更新公式，从
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]
$$
变成了：
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_t + \gamma r_{t+1} + \cdots + \gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t)]
$$

```python
# n-step 的实现多一些这个部分

self.state_list = []  # 保存之前的状态
self.action_list = []  # 保存之前的动作
self.reward_list = []  # 保存之前的奖励

if len(r_list) < n:
  	不足 n 的处理逻辑

for _ in reversed(range(n)):
  	从后往前算, 后面的部分先乘 gamma
    递归更新
```



### 5.5 Q-learning



#### 5.5.1 Q-learning 更新公式推导

Q 函数的贝尔曼方程：
$$
Q(s_t, a_t) = \mathbb{E}[r_{t+1} + \gamma \max_{a{\prime}} Q(s_{t+1}, a{\prime})]
$$
通过交互，得到 $r_{t+1}$，$s_{t+1}$
$$
Q(s_t, a_t) = r_{t+1} + \gamma \max_{a{\prime}} Q(s_{t+1}, a{\prime})
$$
定义 误差 $\delta$
$$
\delta_t = r_{t+1} + \gamma \max_{a{\prime}} Q(s_{t+1}, a{\prime}) - Q(s_t, a_t)
$$

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \delta_t
$$

即：
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a{\prime}} Q(s_{t+1}, a{\prime}) - Q(s_t, a_t) \right]

$$


对比 Sarsa 的更新公式：
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
$$


二者的差别在于，Q-learning 用的是下一个状态的最大 Q 值，而 SARSA 使用的是实际执行的下一个动作的 Q 值



#### 5.5.2 Q-learning 算法

![截屏2025-02-05 00.07.51](/Users/n/Library/Application Support/typora-user-images/截屏2025-02-05 00.07.51.png)



Q 函数的贝尔曼最优方程为：
$$
Q^*(s, a) = r(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) \max_{a'} Q^*(s', a')
$$
为了探索，通常会使用一个 $\epsilon$​-贪婪策略来与环境交互。



#### 5.5.3 On-policy 和 Off-policy 的区别

对于 Sarsa 会在每个时间步，根据当前的策略来选择动作。

即：agent 所执行的动作必须遵循当前策略，无论是探索还是基于 Q-table 选择最优的动作(利用)。
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
$$
Sarsa 使用的是当前策略下实际执行的下一个动作 $a_{t+1}$

即：在更新 Q 值的时候，当前状态下采取的动作和下一个状态实际所执行的动作都是基于当前策略采样的。

在这个公式中，下一步的动作$ a_{t+1} $​必须是当前策略下选择的动作（而不是最大 Q 值对应的动作）。

因此，**SARSA 是在策略方法（on-policy）**，它的学习是基于当前策略的表现。



**Q-learning 是 off-policy 算法**，意味着它不需要使用当前策略来采样数据。

它可以基于与当前策略不同的行为策略（例如使用 $\epsilon$-贪婪）来更新 Q 值。

更新时，它使用的并不是当前策略下实际选择的动作，而是选择下一个状态的 **最优动作** 来进行 Q 值的更新。

**说的简单点：Q-learning 不关心当前策略，而是学习最优策略。通过最大化未来的 Q 值来更新当前的 Q 值。**



采样数据的策略叫做 行为策略(behavior policy)

这些数据用来更新的策略叫做 目标策略 (target policy)



- 对于 on-policy ，行为策略和目标策略是同一个策略

- 对于 off-policy，行为策略和目标策略不是同一个策略

两者的判断区别在于，看计算时序差分的价值目标的数据是否来源于当前的策略。

具体来说：

- 对于 Sarsa，更新公式必须使用来自当前策略采样到的五元组 $(s,a,r,s',a)$，属于 on-policy
- 对于 Q-learning，更新公式使用的是四元组 $(s,a,r,s')$ 来更新当前的价值 $Q(s,a)$ ，这个四元组并不需要一定是当前策略采样到的数据，也可以来自行为策略。

对于 off-policy，能够重复利用过去的训练样本，往往具有更小的样本复杂度。



### 5.6 Sarsa 与 Q-learning 对比

![截屏2025-02-05 01.25.53](/Users/n/Library/Application Support/typora-user-images/截屏2025-02-05 01.25.53.png)



对于 Q-learning 来说，有一点很重要的是，Q-learning 更加的冒险。

这是因为 Q-learning 总是选择理论上最优的动作，即使可能掉悬崖

从长期汇报来看，这条路经更短，回报更高，但是算法在探索，所以 agent 偶尔会选择调入悬崖的动作。

这种 冒险的行为，也会导致训练过程中，Q-leaning 的 return 波动比较大，因为会掉悬崖 -100

总的来说：

- 波动较大，偶尔因为掉入悬崖导致突发性的巨大负奖励

- 长期来看，学到的策略可能更接近理论最优，贴着悬崖走，快速到终点
- Q-learning 理论上更优，但存在探索的高风险



![截屏2025-02-05 01.24.54](/Users/n/Library/Application Support/typora-user-images/截屏2025-02-05 01.24.54.png)

Sarsa 更新依赖于 agent 实际的动作，这意味着 倾向于学习如何避免危险的探索。

在悬崖边的时候，Sarsa 更倾向于选择远离悬崖的路径，即使这条路径长一些，即使回报没有理论上最优的路径高

训练过程中更稳定，智能体不太可能掉入悬崖，导致回报曲线更加平滑。

- 更平稳、期望回报更高，尤其是在训练早期。

- 策略更保守，优先考虑安全性，避免不必要的风险。
- Sarsa 更稳定，适合需要平衡探索和安全性的情况



## Chap6.Dyna-Q 算法

Dyna-Q 同动态规划算法中的策略迭代和价值迭代一样都是 model-based 的算法。

不同点在于 Dyna-Q 的环境模型是通过采样数据估计得到的。



### 6.1 Dyna-Q

DynaQ 的核心思想是：

**在使用真实环境数据进行学习的同时，利用已构建的环境模型进行额外的“虚拟”学习，以提高学习效率。**

Dyna-Q 将强化学习分为三个主要步骤：



1. 真实环境交互

   - agent 与 真实环境交互，获得新的经验数据：$(s,a,r,s')$

   - 使用q_learning 进行常规的 Q 值更新：

   $$
   Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a{\prime}} Q(s{\prime}, a{\prime}) - Q(s, a) \right)
   $$

2. 环境模型学习

   - agent 会讲获得的经验，存储到模型中，通常是 $Model(s,a) \rightarrow (r,s')$
   - 这意味着模型可以预测 **给定状态和动作下** 的奖励 和 下一个状态

3. 基于模型的规划

   - 从已经存储的模型中 **随机采样历史经验**，模拟和环境的交互，进行额外的 Q 值更新。
   - 就好像是 agent 在脑子里 想象学习，不需要再与真实环境交互。



![截屏2025-02-05 13.53.27](/Users/n/Library/Application Support/typora-user-images/截屏2025-02-05 13.53.27.png)



```python
初始化 Q(s, a) 和 Model(s, a)

for each episode:
    初始化状态 s
    while s 不是终止状态:
        1. 选择动作 a（如 ε-贪心策略）
        2. 执行动作 a，观察奖励 r 和下一个状态 s'
        3. Q-learning 更新：
           Q(s, a) ← Q(s, a) + α [r + γ max(Q(s', a')) - Q(s, a)]
        4. 更新：
           Model(s, a) ← (r, s')
        5. 进行 n 次基于模型的规划更新：
            for i in range(n):
                随机从模型中采样 (s_sample, a_sample)
                (r_sample, s'_sample) = Model(s_sample, a_sample)
                Q(s_sample, a_sample) ← Q(s_sample, a_sample) + α [r_sample + γ max(Q(s'_sample, a')) - Q(s_sample, a_sample)]
        6. 更新当前状态：
           s ← s'
```





### 6.2 总结

每次进行一次 Q-learning 后，Dyna-Q 会做 $n$ 次 Q-planing

要强调的是，Dyna-Q 是结合了 model-free 和 model-based 的思想，适用于 **离散且确定性的环境**

- 状态空间 和 动作空间有限，可以列出所有可能的状态和动作
- 且 相同的状态和动作组合总是产生相同的结果



由于环境是 **确定性** 的，意味着：

- **相同的** (s, a) **永远会产生相同的** $(r, s{\prime})$，不需要额外的统计或估计
  - 因此，可以立即更新模型，将这条经验直接存储：

$$
\text{Model}(s, a) \leftarrow (r, s{\prime})
$$

这表明：模型已经 **“学会”了** 在状态 s 下采取动作 a 会发生什么。

在确定性的环境中，模型只需要看到一次经验就能 **记住** 这个规律



与之相对的是随机环境，相同的动作可能产生不同的结果，有概率分布

且需要多条经验估计 转移概率和期望奖励，才能构建准确的模型环境



对于悬崖漫步这个环境中，状态的转移是完全确定的，构建的环境模型的精度最高，所以 当 $n$ 越大，Dyna-Q 收敛的也越快，但在其他环境中，并不一定是这样的。

对于确定的环境，可以通过增加 $n$ 来降低算法的样本复杂度



## Chap7.DQN 算法

当状态空间和动作空间是离散的，可以使用 Q-table 来存储每个 $(s,a)$。

在现实应用，如自动驾驶，状态和动作通常是连续的，Q-table 就不再适用了。

可以考虑用 神经网络来 拟合这个复杂的函数。



**连续动作空间**

若 动作空间是连续的，如机器人控制的角度，可以将 $s$ 和 $a$ 一起输入进神经网络中，输出 $Q(s,a)$
$$
Q(s,a) = f_{\theta}(s,a)
$$


**离散动作空间**

若 动作空间是离散的，可以输入 $s$ ，得到每个可能的动作，向左或向右的 $Q$ 值， $Q(s,a_1),Q(s,a_2)$
$$
Q(s,a)=f_{\theta}(s)
$$


**DQN 就是用来解决连续状态下离散动作的问题。**

CartPole 环境中，状态 $s$​ 是连续的，但是动作 只有向左和向右，动作是离散的。



DQN 用 **经验回放（experience replay）** 和 **目标网络（target network）** 来稳定训练过程

DQN 的目标是最小化损失函数，使得神经网络输出的 Q 值尽量接近目标 Q 值：
$$
L(\theta) = \mathbb{E}\left[\left( Q_{\theta}(s_t, a_t) - \left( r_{t+1} + \gamma \max_{a{\prime}} Q_{\theta^-}(s_{t+1}, a{\prime}) \right) \right)^2\right]
$$
通过优化这个损失函数，神经网络学习到 $Q(s, a)$ 的近似值



### 7.1 DQN

Q-learning 的更新规则
$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a{\prime}} Q(s{\prime}, a{\prime}) - Q(s, a) \right)
$$
这个公式来源于 TD 学习目标 $r + \gamma \max_{a{\prime}} Q(s{\prime}, a{\prime})$ 来增量式更新 $Q(s,a)$

也就是说，要使得 $Q(s,a)$ 和 TD 目标 $r + \gamma \max_{a{\prime}} Q(s{\prime}, a{\prime})$ 越接近。
$$
\omega^* = \arg\min_{\omega} \frac{1}{2N} \sum_{i=1}^{N} \left[ Q_{\omega}(s_i, a_i) - \left( r_i + \gamma \max_{a'} Q_{\omega}(s'_i, a') \right) \right]^2
$$
 对 N 个样本计算平均损失，用 MSE



### 7.2 经验回放

对于监督学习，假设数据是 iid，每个训练数据可能被使用很多次。

但是在原始的 Q-learning 中，每次交互得到的数据 $(s,a,r,s')$ 只会被用一次，然后丢弃

- DQN 引入了经验回放 机制，维护一个 replay buffer，将所有交互的经验存储起来
- 训练 Q 网络时，不直接使用最新的数据，二是从 replay buffer 中随机抽取一批经验进行训练



**经验回放的作用**

1. 打破样本之间的相关性，使其更接近 iid

   **问题：**

   - 在 MDP 中，数据具有时间相关性， $s_t$ 与 $s_{t-1}$ 直接相关，因为 agent 的动作决定了状态的转移。

   - 神经网络在处理这种高度相关的数据时，可能陷入局部最优，导致学习不稳定或发散。

   > 原因：
   >
   > 1. 数据分布不均，会导致模型在短期内只学习到了特定场景下的策略，不能泛化
   >
   > 2. 更重要的原因在于
   >    $$
   >    \theta \leftarrow \theta - \alpha \nabla_{\theta} L(\theta)
   >    $$
   >    对于时间相关的数据，梯度的计算严重依赖于最近的、相似的样本。
   >
   >    **这样会导致梯度的更新方向朝着优化 这些 “特定模式“的数据方向前进，模型被拖进 ”局部最优解“**

   **解决办法：**

   - 随机 抽取 replay buffer 中的样本，打破数据的时间相关性，使得更接近于 iid

   简单来说，就好像是如果只根据昨天的经验做决策，可能会陷入短视，当考虑了过去不同时间的经历，决策会更加全面、更加稳定。



2. 提高样本效率

**问题：**采集新数据的成本很高，若像q_learning 一样，用一次就丢很浪费

**经验回放：**每个经验可以多次使用，network 可以在同一批数据上多次学习，减少了交互次数，降低了训练成本。

简单的说：回顾过去的知识，可以加深对内容的理解，而不是学一次就忘记。



### 7.3 目标网络

**问题：**

在 DQN中，训练的主要挑战之一是不稳定性：

问题在于，Q-learning 的更新公式：
$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a{\prime}} Q(s{\prime}, a{\prime}) - Q(s, a) \right)
$$
在 DQN 中，这个公式变成了损失函数：
$$
L(\theta) = \frac{1}{2} \left( Q_{\theta}(s, a) - \left( r + \gamma \max_{a{\prime}} Q_{\theta}(s{\prime}, a{\prime}) \right) \right)^2
$$

- **自我依赖：** DQN 的目标值 $r + \gamma \max_{a{\prime}} Q_{\theta}(s{\prime}, a{\prime})$ 依赖于当前网络的参数 $\theta$，但是 $\theta$ 正在不断被更新
- **目标和预测同时变化：**很像是，在追逐一个快速移动的目标，追的过程中，目标也在变，训练的过程不稳定。



**解决办法：**

两套神经网络：

**1. 训练网络**，用于生成当前的 $Q_{\theta}(s,a)$，在每次训练中都会被更新

**2.目标网络，** 用于生成 TD 目标值： $ r + \gamma \max_{a{\prime}} Q_{\theta^-}(s{\prime}, a{\prime})$​，参数固定，每隔一段时间才更新一次。
$$
\theta^- \leftarrow \theta
$$

- 由于目标网络的参数在一段时间内保持不变，模型在这段时间内学习的目标是**相对固定的**，减少训练的不稳定
- 当预测值和目标值不再同步快速变化，可以有效防止模型在错误的方向上不断自我强化，避免梯度爆炸或陷入局部最优。

就好像是在射箭

- **没有目标网络：** 每次射箭的时候，靶子都在不断移动，无法稳定提高准度。

- **有目标网络：** 靶子保持静止一段时间，可以稳定练习，等到技巧提升后再调整靶子的位置。



**具体流程**

![截屏2025-02-05 16.52.33](/Users/n/Library/Application Support/typora-user-images/截屏2025-02-05 16.52.33.png)



## Chap9.策略梯度

 DQN 和 Q-learning 都是 value-based 的方法，核心思想是：

1. 学习一个值函数

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a{\prime}} Q(s{\prime}, a{\prime}) - Q(s, a) \right]
$$

2. 根据值函数导出策略，基于 $Q(s,a)$ 选择最优动作

$$
\pi(s) = \arg\max_a Q(s, a)
$$



Policy-base 的核心思想是：

1. 直接学习一个策略 $\pi_{\theta}(s, a)$，即 **显式地 **表示从状态 s 选择动作 a 的概率
2. 不依赖于 值函数，直接优化策略 $\pi_{\theta}$ 使得累积 rewards 最大

$$
J(\theta) = \mathbb{E}{\tau \sim \pi{\theta}} \left[ \sum_{t=0}^{T} R_t \right]
$$

适合连续状态空间，可以探索更加复杂的策略，而不仅仅是选择 $Q$​ 值最大的动作。

Policy-based 方法中，直接学习一个策略函数 $\pi_{\theta}(a \mid s)$。

为了能够优化，需要用参数化的模型，比如 nn 来表示策略
$$
\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)
$$


### 9.1 策略梯度的推导

目标是找到最优策略 $\pi_{\theta}$，使得期望累积奖励最大
$$
J(\theta) = \mathbb{E}{\tau \sim \pi{\theta}} \left[ \sum_{t=0}^{T} R_t \right]
$$
其中，$ \tau = (s_0, a_0, s_1, a_1, …, s_T, a_T) $ 是一条完整的轨迹，$	 \mathbb{E}{\tau \sim \pi{\theta}} $ 表示轨迹 $\tau$ 按照策略 $\pi_{\theta}$ 采样的期望。

对参数 $\theta$ 计算梯度：
$$
\nabla_{\theta} J(\theta) = \nabla_{\theta} \mathbb{E}{\tau \sim \pi{\theta}} \left[ \sum_{t=0}^{T} R_t \right]
$$
**轨迹** $\tau$ **的分布** $P_{\theta}(\tau) $ 依赖于策略 $\pi_{\theta}$ **，所以不能直接求导！**



利用概率分布的期望梯度公式：
$$
\nabla_{\theta} \mathbb{E}{X}[f(X)] = \mathbb{E}{X} [\nabla_{\theta} f(X)]
$$

$$
\nabla_{\theta} J(\theta) = \mathbb{E}{\tau \sim \pi{\theta}} \left[ \nabla_{\theta} \sum_{t=0}^{T} R_t \right]
$$

**奖励** $R_t$ **并不依赖于** $\theta$ ，它是环境给出的反馈，和策略的参数 $\theta$ 没有直接关系。



在概率分布  $P_{\theta}(X)$  下，期望值的梯度有一个常见技巧：
$$
\nabla_{\theta} \mathbb{E}{X \sim P{\theta}}[f(X)] = \mathbb{E}{X \sim P{\theta}}[\nabla_{\theta} f(X)]
$$
如果  $f(X)$  不直接依赖于  $\theta$ ，但  $X$  是从依赖于  $\theta$  的分布 $ P_{\theta}(X) $ 采样，那么：
$$
\nabla_{\theta} \mathbb{E}{X \sim P{\theta}} [f(X)] = \mathbb{E}{X \sim P{\theta}} [ f(X) \nabla_{\theta} \log P_{\theta}(X) ]
$$
这个式子是**对数导数技巧**



**应用到策略梯度：**
$$
\nabla_{\theta} J(\theta) = \mathbb{E}{\tau \sim \pi{\theta}} \left[ \sum_{t=0}^{T} R_t \nabla_{\theta} \log P_{\theta}(\tau) \right]
$$
其中：$P_{\theta}(\tau)$ **是轨迹的概率**，它由策略 $\pi_{\theta}$ 和环境动态 $P(s{\prime}|s,a)$ 决定：
$$
P_{\theta}(\tau) = p(s_0) \prod_{t=0}^{T} \pi_{\theta}(a_t | s_t) P(s_{t+1} | s_t, a_t)
$$

-  $\pi_{\theta}(a_t | s_t)$ **依赖于** $\theta$ 
- $P(s_{t+1} | s_t, a_t)$ **由环境决定，不依赖** $\theta$ 

对轨迹概率 $P_{\theta}(\tau)$ 取对数：
$$
\log P_{\theta}(\tau) = \sum_{t=0}^{T} \log \pi_{\theta}(a_t | s_t) + \sum_{t=0}^{T} \log P(s_{t+1} | s_t, a_t)
$$
由于环境转移概率 $P(s_{t+1}\mid s_t,a_t)$ 不依赖于 $\theta$ 梯度为0，则有
$$
\nabla_{\theta} \log P_{\theta}(\tau) = \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)
$$
代回策略梯度公式：
$$
\nabla_{\theta} J(\theta) = \mathbb{E}{\tau \sim \pi{\theta}} \left[ \sum_{t=0}^{T} R_t \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \right]
$$
交换求和顺序：
$$
\nabla_{\theta} J(\theta) = \mathbb{E}{\tau \sim \pi{\theta}} \left[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R_t \right]
$$


#### 对数导数技巧的证明



假设 $P_{\theta}(X)$ 是关于 X 的概率分布，定义其梯度：
$$
\nabla_{\theta} P_{\theta}(X) = P_{\theta}(X) \nabla_{\theta} \log P_{\theta}(X)
$$
这个式子是由对数求导得到的：
$$
\nabla_{\theta} \log P_{\theta}(X) = \frac{\nabla_{\theta} P_{\theta}(X)}{P_{\theta}(X)}
$$
然后，对**期望值** $\mathbb{E}{X \sim P{\theta}} [f(X)]$ 求梯度：
$$
\nabla_{\theta} \mathbb{E}{X \sim P{\theta}} [f(X)]
= \nabla_{\theta} \int P_{\theta}(X) f(X) dX
$$

$$
= \int \nabla_{\theta} P_{\theta}(X) f(X) dX
$$

$$
= \int P_{\theta}(X) \nabla_{\theta} \log P_{\theta}(X) f(X) dX
$$

把 $P_{\theta}(X)$ 提出到期望符号外：
$$
= \mathbb{E}{X \sim P{\theta}} [ f(X) \nabla_{\theta} \log P_{\theta}(X) ]
$$
得到：
$$
\nabla_{\theta} \mathbb{E}{X \sim P{\theta}} [f(X)] = \mathbb{E}{X \sim P{\theta}} [ f(X) \nabla_{\theta} \log P_{\theta}(X) ]
$$


### 9.2 REINFORCE

$$
\nabla_{\theta} J(\theta) = \mathbb{E}{\pi{\theta}} \left[ \sum_{t=0}^{T} \left( \sum_{t{\prime}=t}^{T} \gamma^{t{\prime}-t} r_{t{\prime}} \right) \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \right]
$$

REINFORCE 通过 MC 采样来估计策略梯度，基本流程如下：

![截屏2025-02-07 01.40.29](/Users/n/Library/Application Support/typora-user-images/截屏2025-02-07 01.40.29.png)

1. 初始化 $\theta$
2. 采样轨迹：运行当前的策略 $\pi_{\theta}$ ，收集多个完整的 episode
3. 对每个轨迹计算回报 $R_t$

$$
R_t = \sum_{k=t}^{T} \gamma^{k-t} r_k
$$

4. 计算梯度，更新参数

$$
\theta \leftarrow \theta + \alpha \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R_t
$$

5. 重复，直到收敛









